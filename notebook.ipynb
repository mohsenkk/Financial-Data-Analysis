{
 "cells": [
  {
   "attachments": {
    "hermes_logo.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "5f7ca924",
   "metadata": {},
   "source": [
    "![hermes_logo.png](attachment:hermes_logo.png)\n",
    "<p><center><font size=\"5\" face=\"Times New Roman\" color=\"#3b5996\"><b>Financial Data Analyst Entry Task<br>Hermes Capital Quant Chapter<br>Fall 2023</b></font></center></p>\n",
    "<center><font size=\"4\" face=\"Times New Roman\" color=\"#3b5996\">Mohsen Kashefikia</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1fcddc",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    " First we Install python-binance for API interaction with <b>Binance</b>:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f862afe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install python-binance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8198e",
   "metadata": {},
   "source": [
    "<p><font face=\"gotham\"></font></p>\n",
    "\n",
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    " Before starting tasks, we import the necessary libraries:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6a55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import kurtosis, skew, probplot, shapiro\n",
    "from statsmodels.tsa.stattools import pacf, acf, adfuller\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly import subplots\n",
    "from plotly.offline import iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from binance.client import Client\n",
    "from binance.exceptions import BinanceAPIException\n",
    "\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14e1324",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color: #3b5996; padding: 10px;border-radius: 25px;\"><center><font face=\"gotham\" color=\"white\">Task 1: Data Collection, Processing, and Resampling</font></center><h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67bac06",
   "metadata": {},
   "source": [
    "<h2><font face=\"gotham\" color=\"#3b5996\">1.1 Data Collection: Fetching OHLCV Candlestick Data</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1374caa",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">1. Modular Python Functions for API Interactions</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aabffa5",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    " As the first step, we develop a class named <i>BinanceDataHandler</i> for fetching OHLCV data from <b>Binance</b> and saving it:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd111e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unix_to_datetime(unix_time):\n",
    "    \"\"\"\n",
    "    Convert Unix timestamp to datetime format.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    unix_time : int\n",
    "        Unix timestamp in milliseconds.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    datetime.datetime\n",
    "        Datetime representation of the Unix timestamp.\n",
    "    \"\"\"\n",
    "    # Convert Unix timestamp from milliseconds to seconds\n",
    "    return datetime.fromtimestamp(unix_time / 1000.0)\n",
    "\n",
    "\n",
    "class BinanceDataHandler:\n",
    "    \"\"\"\n",
    "    A class to handle fetching and saving Binance data.\n",
    "\n",
    "    Attributes:\n",
    "    ----------\n",
    "    client : binance.client.Client\n",
    "        Client object for interacting with the Binance API.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key, api_secret):\n",
    "        \"\"\"\n",
    "        Constructs all the necessary attributes for the BinanceDataHandler object.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        api_key : str\n",
    "            API key for Binance.\n",
    "        api_secret : str\n",
    "            API secret for Binance.\n",
    "        \"\"\"\n",
    "        # Initialize the Binance client with API key and secret\n",
    "        self.client = Client(api_key, api_secret)\n",
    "        self.exchange_name = 'binance'\n",
    "\n",
    "    def fetch_historical_data(self, symbol, interval, start_str, end_str, delta_weeks=1):\n",
    "        \"\"\"\n",
    "        Fetch OHLCV data from Binance and return it as a pandas DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        symbol : str\n",
    "            Exchange pair symbol (e.g., 'BTCUSDT').\n",
    "        interval : str\n",
    "            Time interval for the data (e.g., '1h', '1d').\n",
    "        start_str : str\n",
    "            Starting datetime in string format (e.g., '2021-01-01').\n",
    "        end_str : str\n",
    "            Ending datetime in string format (e.g., '2021-01-31').\n",
    "        delta_weeks : int, optional\n",
    "            Number of weeks for each data fetching interval, by default 1.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        pandas.DataFrame\n",
    "            DataFrame containing the fetched OHLCV data.\n",
    "        \"\"\"\n",
    "        # Parse the start and end dates from strings to datetime objects\n",
    "        start_date = parse(start_str)\n",
    "        end_date = parse(end_str)\n",
    "        # Define the time interval for data fetching\n",
    "        delta = timedelta(weeks=delta_weeks)\n",
    "\n",
    "        all_klines = []\n",
    "\n",
    "        # Fetch data in chunks defined by delta until the end date is reached\n",
    "        while start_date < end_date:\n",
    "            start = start_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            end = (start_date + delta).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            while True:\n",
    "                try:\n",
    "                    # Fetch historical data from Binance\n",
    "                    klines = self.client.get_historical_klines(\n",
    "                        symbol, interval, start, end\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"Historical data from {start} until {end} fetched successfully.\"\n",
    "                    )\n",
    "                    time.sleep(1)  # Pause to respect API rate limits\n",
    "                    break\n",
    "                except BinanceAPIException as e:\n",
    "                    # Handle API exceptions and retry after a cooldown period\n",
    "                    print(\"Error occurred!\")\n",
    "                    print(f\"StatusCode = {e.status_code}, Message = {e.message}\")\n",
    "                    print(\"Cooling down for 10 seconds...\")\n",
    "                    time.sleep(10)\n",
    "            # Append fetched data to the list\n",
    "            all_klines.extend(klines)\n",
    "            # Move to the next time interval\n",
    "            start_date += delta\n",
    "\n",
    "        # Convert raw data to a DataFrame\n",
    "        ohlc_data = [\n",
    "            [\n",
    "                unix_to_datetime(kline[0]),\n",
    "                float(kline[1]),\n",
    "                float(kline[2]),\n",
    "                float(kline[3]),\n",
    "                float(kline[4]),\n",
    "                float(kline[5]),\n",
    "                'binance',\n",
    "                symbol,\n",
    "            ]\n",
    "            for kline in all_klines\n",
    "        ]\n",
    "        df = pd.DataFrame(\n",
    "            ohlc_data,\n",
    "            columns=[\n",
    "                'datetime',\n",
    "                'open',\n",
    "                'high',\n",
    "                'low',\n",
    "                'close',\n",
    "                'volume',\n",
    "                'exchange',\n",
    "                'isin',\n",
    "            ],\n",
    "        )\n",
    "        # Filter out data beyond the end date\n",
    "        df = df[~(df['datetime'] > end_date.strftime('%Y-%m-%d %H:%M:%S'))]\n",
    "        return df\n",
    "\n",
    "    def save_historical_data(self, df, dataset_folder, file_name):\n",
    "        \"\"\"\n",
    "        Save a pandas DataFrame to a CSV file.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df : pandas.DataFrame\n",
    "            DataFrame containing the OHLCV data.\n",
    "        dataset_folder : str\n",
    "            Directory path where the CSV file will be saved.\n",
    "        file_name : str\n",
    "            Name of the CSV file to save.\n",
    "        \"\"\"\n",
    "        # Save DataFrame to a CSV file in the specified directory\n",
    "        df.to_csv(f'{dataset_folder}/{file_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222c40a",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    " As the next step, we fetch data from the requested start time until the requested end time, mentioned in the task description(Since we fetch public data from <b>Binance</b>, we don't need <i>api_key</i> and <i>api_secret</i>)\n",
    "</font></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c2b308",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the BinanceDataHandler\n",
    "bdh = BinanceDataHandler(\"\", \"\")\n",
    "\n",
    "# Fetch historical data for BTCUSDT with a 1-minute interval from the specified date range\n",
    "btcusdt_binance_df = bdh.fetch_historical_data(\n",
    "    'BTCUSDT', '1m', \"2022-11-30 00:00:00\", \"2023-12-02 00:00:00\", 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba31ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "btcusdt_binance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca69fd8",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    " We save data fetched in previous script in csv format:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4723247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fetched data to a CSV file\n",
    "bdh.save_historical_data(btcusdt_binance_df, 'dataset', 'btcusdt_m1_binance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e1c487",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    Now we read all csv files, save <b>OHLCV</b> data in <i>pandas DataFrame</i> format .In the next step we drop all duplicate records ,sort all <i>DataFrames</i> by <i>datetime</i> and set <i>datetime</i> column as index:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4792d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the folder where datasets are stored\n",
    "dataset_folder = 'dataset'\n",
    "\n",
    "# Define the date range for the data extraction\n",
    "start_date = '2022-12-01 00:00:00'\n",
    "end_date = '2023-12-01 00:00:00'\n",
    "\n",
    "# List of exchange pairs to process\n",
    "exchange_pairs = [\n",
    "    'btcusdt-binance', \n",
    "    'usdttmn-wallex', \n",
    "    'usdttmn-tabdeal', \n",
    "    'usdttmn-nobitex', \n",
    "    'btctmn-wallex', \n",
    "    'btctmn-tabdeal', \n",
    "    'btctmn-nobitex'\n",
    "]\n",
    "\n",
    "# Dictionary to store processed data\n",
    "data = dict()\n",
    "\n",
    "def sort_pair_in_requested_dates(dataset_folder, file_name, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Read and sort data for a specific exchange pair within the requested date range.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset_folder : str\n",
    "        The folder where the dataset files are stored.\n",
    "    file_name : str\n",
    "        The name of the CSV file containing the data for the exchange pair.\n",
    "    start_date : str\n",
    "        The start date for the data extraction in 'YYYY-MM-DD HH:MM:SS' format.\n",
    "    end_date : str\n",
    "        The end date for the data extraction in 'YYYY-MM-DD HH:MM:SS' format.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    pandas.DataFrame\n",
    "        The DataFrame containing sorted data within the requested date range.\n",
    "    \"\"\"\n",
    "    # Load the data from the CSV file\n",
    "    df = pd.read_csv(f'{dataset_folder}/{file_name}')\n",
    "    \n",
    "    # Drop duplicate rows based on the 'datetime' column, keeping the first occurrence\n",
    "    df = df.drop_duplicates(subset=['datetime'], keep='first')\n",
    "    \n",
    "    # Filter rows to include only those within the specified date range\n",
    "    df = df[(start_date < df['datetime']) & (df['datetime'] < end_date)]\n",
    "    \n",
    "    # Sort the DataFrame by the 'datetime' column\n",
    "    df = df.sort_values(by='datetime')\n",
    "    \n",
    "    # Convert 'datetime' column to datetime type and set it as the index\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process each exchange pair and store the result in the 'data' dictionary\n",
    "for exchange_pair in exchange_pairs:\n",
    "    # Construct the file name by joining parts of the exchange pair with '_m1_'\n",
    "    file_name = f'{\"_m1_\".join(exchange_pair.split(\"-\"))}.csv'\n",
    "    \n",
    "    # Sort and filter data for the exchange pair within the specified date range\n",
    "    data[exchange_pair] = sort_pair_in_requested_dates(dataset_folder, file_name, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2987b00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['btcusdt-binance']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95333048",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">2. Processing OHLCV Data into a Representative Price Series</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0594817f",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    We create a function named <i>ohlcv_to_price_series</i> to change ohlcv format to a price series:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohlcv_to_price_series(df, method):\n",
    "    \"\"\"\n",
    "    Convert OHLCV DataFrame to a price series based on the specified method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing OHLCV data with columns 'open', 'high', 'low', 'close', 'volume'.\n",
    "    method : str\n",
    "        Method to extract the price series (e.g. 'close' for closing prices).\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    pandas.DataFrame or None\n",
    "        DataFrame with a single 'price' column if the method is recognized,\n",
    "        otherwise None.\n",
    "    \"\"\"\n",
    "    # Use a match-case statement to determine the method for extracting the price series\n",
    "    match method:\n",
    "        case \"close\":\n",
    "            # Extract the 'close' column and rename it to 'price'\n",
    "            return df[[\"close\"]].rename(columns={'close': 'price'})\n",
    "        case _:\n",
    "            # Return None if the method is not recognized\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f4c994",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   We choose <i>close</i> price as the price series value in convertion since:\n",
    "    <ol>\n",
    "        <li><b>Simplicity</b>: It is straightforward to use and easy to understand.</li> \n",
    "        <li><b>Commonly Used</b>: Many traders and analysts use the closing price for daily charts, making it a standard for comparison.</li>\n",
    "        <li><b>Reflects Final Sentiment</b>: It reflects the final consensus of value for that period.</li>\n",
    "    </ol>\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105aeb92",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   Now we imply convertion on all data we have:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88275a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each exchange pair in the data dictionary\n",
    "for exchange_pair in data.keys():\n",
    "    # Convert OHLCV data to a price series using the 'close' method\n",
    "    data[exchange_pair] = ohlcv_to_price_series(data[exchange_pair], method='close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1208b1d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data['btctmn-nobitex']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21988480",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">3. Extraction of Implied USDT-TMN Price Series:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb554cf6",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   In this section First we create a function for finding number of gaps and maximum gap for each exchange pair:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3934bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minute_gap_info(df):\n",
    "    \"\"\"\n",
    "    Find the maximum gap between consecutive timestamps in minutes and the number of such gaps in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing data with a datetime index.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    tuple\n",
    "        A tuple containing the number of gaps and the maximum gap between consecutive timestamps in minutes.\n",
    "    \"\"\"\n",
    "    # Calculate the time difference between consecutive timestamps in minutes\n",
    "    time_diff = df.index.to_series().diff().dt.total_seconds() / 60\n",
    "    \n",
    "    # Select time differences greater than 1 minute (indicating a gap)\n",
    "    gaps = time_diff.loc[lambda x: x > 1]\n",
    "    \n",
    "    # Return the number of gaps and the maximum gap\n",
    "    return len(gaps), max(gaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003881f0",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   Now for each pair we print gaps information to decide which pairs are better to use:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5f943",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for exchange_pair in data.keys():\n",
    "    # Find the number of gaps and the maximum gap for each exchange pair\n",
    "    num_gaps, max_gap = find_minute_gap_info(data[exchange_pair])\n",
    "    \n",
    "    # Print the results for each exchange pair\n",
    "    print(f\"Number of gaps for {exchange_pair}: {num_gaps}, Maximum gap: {max_gap} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ffaba",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   As we see among all Iranian exchanges, <b>Nobitex</b> exchange data has the highest quality since having fewer missing records and <b>Tabdeal</b> exchange has the least quality since having the most missing records.<br> So for calculating the <i>USDT-TMN</i> pair we use <b>Nobitex</b> data for the <i>BTC-TMN</i> pair and <b>Binance</b> data for the <i>BTC-USDT</i> pair:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f085f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge the data from 'btctmn-nobitex' and 'btcusdt-binance' on their indices\n",
    "merged_df = pd.merge(data['btctmn-nobitex'], data['btcusdt-binance'], left_index=True, right_index=True)\n",
    "\n",
    "# Calculate the new 'price' column as the ratio of 'price_x' to 'price_y'\n",
    "merged_df[\"price\"] = merged_df[\"price_x\"] / merged_df[\"price_y\"]\n",
    "\n",
    "# Create a new entry in the data dictionary with the calculated 'price'\n",
    "# Drop the original 'price_x' and 'price_y' columns from the merged DataFrame\n",
    "data['usdttmn-calculated'] = merged_df.drop(['price_x', 'price_y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['usdttmn-calculated']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c62731",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   At this section we choose these USDT-TMN price series for future parts:\n",
    "<ol>\n",
    "    <li><b>usdttmn-wallex</b>: <i>USDT-TMN</i> price series extracted from wallex</li>\n",
    "  <li><b>usdttmn-nobitex</b>: <i>USDT-TMN</i> price series extracted from nobitex</li>\n",
    "  <li><b>usdttmn-calculated</b>: <i>USDT-TMN</i> price series calculated of division <i>BTC-USDT</i> extracted from binance by <i>BTC-TMN</i> extracted from nobitex</li>\n",
    "</ol>\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd85c0fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# List of chosen exchange pairs to keep in the data dictionary\n",
    "chosen_data = ['usdttmn-wallex', 'usdttmn-nobitex', 'usdttmn-calculated']\n",
    "\n",
    "# Filter the data dictionary to include only the chosen exchange pairs\n",
    "data = {exchange_pair: data[exchange_pair] for exchange_pair in chosen_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a564b0",
   "metadata": {},
   "source": [
    "<h2><font face=\"gotham\" color=\"#3b5996\">1.2 Resampling</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ef5cb2",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">1. Selection of Time Scales:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96040545",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   First, we create a deep copy of <i>data</i> to prevent unwanted changes on the previous part data and name it <i>resampled_data</i>:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da95a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep copy of the data dictionary to prevent modifications to the original data\n",
    "resampled_data = deepcopy(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c392e32b",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   Now we create a function for resampling price for each exchange pair:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c54e09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def resample_data(price_series, freq, method):\n",
    "    \"\"\"\n",
    "    Resample the price series DataFrame to a specified frequency using the specified method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    price_series : pandas.DataFrame\n",
    "        DataFrame containing the price series with a datetime index.\n",
    "    freq : str\n",
    "        The new frequency to resample the data to (e.g., 'D' for daily, 'H' for hourly).\n",
    "    method : str\n",
    "        Method for resampling ('last' to take the last value in each resampled period).\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    pandas.DataFrame or None\n",
    "        Resampled DataFrame if the method is recognized, otherwise None.\n",
    "    \"\"\"\n",
    "    match method:\n",
    "        case 'last':\n",
    "            # Resample the price series to the specified frequency using the last value in each period\n",
    "            return price_series.resample(freq).last()\n",
    "        case _:\n",
    "            # Return None if the method is not recognized\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a8980",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   We create new sampled data by resampling previous section data with given frequencies:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = ['5min', '20min', '60min', '1440min']\n",
    "\n",
    "# Loop through each exchange pair in the resampled_data dictionary\n",
    "for exchange_pair in resampled_data.keys():\n",
    "    # Dictionary to store resampled data for different frequencies\n",
    "    frequencies_data = dict()\n",
    "    \n",
    "    # Loop through each frequency and resample the data\n",
    "    for freq in frequencies:\n",
    "        # Resample the data using the 'ffill' method (forward fill)\n",
    "        frequencies_data[freq] = resample_data(resampled_data[exchange_pair], freq, 'last')\n",
    "    \n",
    "    # Update the resampled_data dictionary with the new frequencies data\n",
    "    resampled_data[exchange_pair] = frequencies_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d6d48",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">2. Methodological Approach:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbfad15",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   Since we are using <b>usdttmn-calculated</b> we can not calculate volume with extracted data in previous part, we can't use the <b>VWAP</b> approach for resampling. Furthermore, the <b>TWAP</b> approach is suitable for Long-Term Trends. As a result, <b>using the last record price</b> might be most suitable since:\n",
    "    <ol>\n",
    "        <li><b>Simplicity</b>: Extremely easy to understand and calculate.</li> \n",
    "        <li><b>Real-time Relevance</b>: Represents the most recent market conditions at the end of each interval.</li>\n",
    "    </ol>\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8e6168",
   "metadata": {},
   "source": [
    "<h2><font face=\"gotham\" color=\"#3b5996\">1.3 Handling Market Anomalies</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c361b05",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">1. Missing Data:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be742c",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   First, we create a deep copy of <i>resampled_data</i> to prevent unwanted changes on the previous part data and name it <i>complete_data</i>:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4039c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep copy of the resampled_data dictionary to prevent modifications to the original data\n",
    "complete_data = deepcopy(resampled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d2102",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   Now we create a function for filling missed price for each exchange pair:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d83ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missed_data(price_series, method):\n",
    "    \"\"\"\n",
    "    Fill missing data in the price series using the specified method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    price_series : pandas.DataFrame or pandas.Series\n",
    "        DataFrame or Series containing the price data with a datetime index.\n",
    "    method : str\n",
    "        Method to fill missing data ('linear interpolate' for linear interpolation, 'ffill' for forward fill).\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    pandas.DataFrame or pandas.Series or None\n",
    "        The DataFrame or Series with missing data filled if the method is recognized, otherwise None.\n",
    "    \"\"\"\n",
    "    match method:\n",
    "        case 'linear interpolate':\n",
    "            # Fill missing data using linear interpolation\n",
    "            return price_series.interpolate(method='linear')\n",
    "        case 'ffill':\n",
    "            # Fill missing data using forward fill\n",
    "            return price_series.ffill()\n",
    "        case _:\n",
    "            # Return None if the method is not recognized\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36115609",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   We fill missed data of all frequencies for each exchange pair:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c93e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each exchange pair in the data dictionary\n",
    "for exchange_pair in complete_data.keys():\n",
    "    # Loop through each frequency within the exchange pair data\n",
    "    for freq in complete_data[exchange_pair].keys():\n",
    "        # Fill missing data using the 'linear interpolate' method\n",
    "        complete_data[exchange_pair][freq] = fill_missed_data(complete_data[exchange_pair][freq], 'linear interpolate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c371806",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">2. Outlier Detection and Correction:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb51aa2",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    We choose zscore method for outlier detection. Fisrt, we define a <i>zscore</i> function used rolling window for outlier detection and replace outlier with rolling average:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424dae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def zscore(df, window, thresh=3, return_all=False):\n",
    "    \"\"\"\n",
    "    Calculate the z-score for the given DataFrame, and optionally return the mean, standard deviation, \n",
    "    and a mask of values within the specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame or pandas.Series\n",
    "        The input data to calculate the z-score for.\n",
    "    window : int\n",
    "        The rolling window size to calculate the mean and standard deviation.\n",
    "    thresh : float, optional (default=3)\n",
    "        The z-score threshold to identify outliers. Values with z-score beyond this threshold will be considered outliers.\n",
    "    return_all : bool, optional (default=False)\n",
    "        If True, return the z-score, mean, standard deviation, and mask. Otherwise, return the DataFrame with outliers replaced by the mean.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    pandas.DataFrame or pandas.Series, tuple\n",
    "        The DataFrame with outliers replaced by the mean if return_all is False.\n",
    "        If return_all is True, return a tuple containing the z-score, mean, standard deviation, and mask.\n",
    "    \"\"\"\n",
    "    # Calculate rolling mean and standard deviation\n",
    "    roll = df.rolling(window=window, min_periods=1, center=True)\n",
    "    avg = roll.mean()\n",
    "    std = roll.std(ddof=0)\n",
    "    \n",
    "    # Calculate z-score\n",
    "    z = df.sub(avg).div(std)\n",
    "    \n",
    "    # Create a mask for values within the threshold\n",
    "    m = z.between(-thresh, thresh)\n",
    "    \n",
    "    if return_all:\n",
    "        return z, avg, std, m\n",
    "    \n",
    "    # Replace outliers with the mean\n",
    "    return df.where(m, avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218d6764",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    Now we plot all price sereies with their rolling average. Furthermore, we specify all outlier points with their replacemen:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a026f04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_figure_of_missing_data_and_replacement(exchange_pair, freq, price_series, zscore_info):\n",
    "    \"\"\"\n",
    "    Create traces for a Plotly figure to visualize missing data and its replacement.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    exchange_pair : str\n",
    "        The exchange pair being processed.\n",
    "    freq : str\n",
    "        The frequency of the data.\n",
    "    price_series : pandas.DataFrame\n",
    "        DataFrame containing the price series data.\n",
    "    zscore_info : tuple\n",
    "        A tuple containing z-score, mean, standard deviation, and mask of valid values.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    list\n",
    "        A list of Plotly Scatter traces.\n",
    "    \"\"\"\n",
    "    z, avg, std, m = zscore_info\n",
    "    x1 = price_series.index.values\n",
    "    x2 = avg[~m].index.values\n",
    "\n",
    "    # Trace for the original data\n",
    "    trace1 = go.Scatter(\n",
    "        x=x1,\n",
    "        y=price_series.price.values,\n",
    "        marker=dict(color=\"blue\"),\n",
    "        name='data'\n",
    "    )\n",
    "\n",
    "    # Trace for the rolling mean\n",
    "    trace2 = go.Scatter(\n",
    "        x=x1,\n",
    "        y=avg.values,\n",
    "        marker=dict(color=\"red\"),\n",
    "        name='mean'\n",
    "    )\n",
    "\n",
    "    # Trace for the detected outliers\n",
    "    trace3 = go.Scatter(\n",
    "        x=x2,\n",
    "        y=price_series.loc[~m, 'price'].values,\n",
    "        marker=dict(color=\"green\", size=5),\n",
    "        mode=\"markers\",\n",
    "        name='outliers'\n",
    "    )\n",
    "\n",
    "    # Trace for the replacement values\n",
    "    trace4 = go.Scatter(\n",
    "        x=x2,\n",
    "        y=avg[~m].values,\n",
    "        marker=dict(color=\"gold\", size=5),\n",
    "        mode=\"markers\",\n",
    "        name='replacement'\n",
    "    )\n",
    "\n",
    "    return [trace1, trace2, trace3, trace4]\n",
    "\n",
    "def plot_figures_for_each_exchange_pair(data, exchange_pair, windows):\n",
    "    \"\"\"\n",
    "    Plot figures for each exchange pair and frequency, showing missing data handling.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : dict\n",
    "        Dictionary containing the price data for different exchange pairs and frequencies.\n",
    "    exchange_pair : str\n",
    "        The exchange pair being processed.\n",
    "    windows : dict\n",
    "        Dictionary containing the window sizes for different frequencies.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    figures = []\n",
    "    subplot_titles = []\n",
    "\n",
    "    for freq in data[exchange_pair].keys():\n",
    "        price_series = data[exchange_pair][freq]\n",
    "        zscore_info = zscore(price_series.price, window=windows[freq], return_all=True)\n",
    "\n",
    "        # Add title for each subplot\n",
    "        subplot_titles.append(f\"time-frame-interval = {freq}\")\n",
    "\n",
    "        # Create figure for each frequency\n",
    "        figures.append(create_figure_of_missing_data_and_replacement(exchange_pair, freq, price_series, zscore_info))\n",
    "\n",
    "    # Create subplots for each frequency\n",
    "    fig = subplots.make_subplots(rows=len(figures), cols=1, subplot_titles=tuple(subplot_titles))\n",
    "\n",
    "    # Add each trace to the subplot\n",
    "    for i, figure in enumerate(figures):\n",
    "        for trace in figure:\n",
    "            fig.add_trace(trace, row=i+1, col=1)\n",
    "            fig.update_xaxes(title_text=\"datetime\", row=i+1, col=1)\n",
    "            fig.update_yaxes(title_text=\"price\", row=i+1, col=1)\n",
    "\n",
    "    fig.update_layout(title_text=f'Missing data handling for {exchange_pair}', height=1000)\n",
    "    iplot(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168ad18",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    We set three days as window for rolling average.\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f63706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window sizes for different frequencies\n",
    "windows = {'5min': 864, '20min': 216, '60min': 72, '1440min': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dab84e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loop through each exchange pair in the complete data\n",
    "for exchange_pair in complete_data.keys(): \n",
    "    plot_figures_for_each_exchange_pair(complete_data, exchange_pair, windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00254ec1",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">3. Data Integrity Assurance:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec3828",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    First, we define a function named <i>print_data_metrics</i> that prints all frequency data metrics for each exchange pair:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cbc4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_metrics(data_exchange_pair):\n",
    "    \"\"\"\n",
    "    Generate and print descriptive statistics for each frequency of a given exchange pair.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_exchange_pair : dict\n",
    "        Dictionary containing the price data for different frequencies of an exchange pair.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame containing the concatenated descriptive statistics for each frequency.\n",
    "    \"\"\"\n",
    "    data_info = []\n",
    "    for freq in data_exchange_pair.keys():\n",
    "        # Generate descriptive statistics for the current frequency\n",
    "        data_info.append(data_exchange_pair[freq].describe())\n",
    "    \n",
    "    # Concatenate descriptive statistics for all frequencies into a single DataFrame\n",
    "    return pd.concat(data_info, axis=1, keys=list(data_exchange_pair.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d5c30c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_data_metrics(complete_data['usdttmn-wallex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data_metrics(complete_data['usdttmn-nobitex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db55f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_data_metrics(complete_data['usdttmn-calculated'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febf2572",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    Now we print all frequency metrics for each exchange pair again after implying zscore method for outlier detection and replacement:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573cff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize price data using z-score for each exchange pair and frequency\n",
    "for exchange_pair in complete_data.keys():\n",
    "    for freq in complete_data[exchange_pair].keys():\n",
    "        # Update the price column with z-score normalized values\n",
    "        complete_data[exchange_pair][freq]['price'] = zscore(complete_data[exchange_pair][freq]['price'], window=windows[freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df4f661",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_data_metrics(complete_data['usdttmn-wallex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a6d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_data_metrics(complete_data['usdttmn-nobitex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6020374",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_data_metrics(complete_data['usdttmn-calculated'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b962cbd6",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    As we see after applying the zscore method, our data distribution does not change significantly. If we look at important metrics of data distribution like <b>mean</b>, <b>std</b>, <b>min</b>, <b>max</b>, and <b>quantiles</b> we see no big difference after outlier replacement. Furthermore, we can see our data cleaning process preserves data consistency successfully since we don't drop outliers and replace them with suitable records.\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9d3ea",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color: #3b5996; padding: 10px;border-radius: 25px;\"><center><font face=\"gotham\" color=\"white\">Task 2: Exploratory Data Analysis (EDA)</font></center><h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf22e26",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   First, we create a deep copy of <i>complete_data</i> to prevent unwanted changes on the previous part data and name it <i>clean_data</i>:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5184a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deep copy of the data dictionary to prevent modifications to the original data\n",
    "clean_data = deepcopy(complete_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada6ecd",
   "metadata": {},
   "source": [
    "<h2><font face=\"gotham\" color=\"#3b5996\">2.1 Log Returns, Volatility, and Normality Assessment</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5be4a",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">1. Log Returns Computation:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d499a0",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   We create a function named <i>calculate_log_return</i> to calculate log return with prices:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_return(df, col='price', periods=1):\n",
    "    \"\"\"\n",
    "    Calculate the logarithmic return of a specified column in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input data frame containing the price data.\n",
    "    col : str, optional (default='price')\n",
    "        The column name for which to calculate the logarithmic return.\n",
    "    periods : int, optional (default=1)\n",
    "        The number of periods to shift for calculating the return.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    pandas.DataFrame\n",
    "        The input DataFrame with an additional 'log_return' column containing the logarithmic returns.\n",
    "    \"\"\"\n",
    "    # Calculate the natural logarithm of the specified column\n",
    "    log_df = np.log(df[col])\n",
    "\n",
    "    # Calculate the difference between the log values and their shifted version\n",
    "    df[\"log_return\"] = log_df - log_df.shift(periods=periods)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf988c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the log return calculation to each exchange pair and frequency in the clean_data dictionary\n",
    "for exchange_pair in clean_data.keys():\n",
    "    for freq in clean_data[exchange_pair].keys():\n",
    "        clean_data[exchange_pair][freq] = calculate_log_return(clean_data[exchange_pair][freq])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867d9685",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">2. Volatility Estimation and Clustering Analysis with EWMA:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133cb311",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   We define a function named <i>calculate_ewma_volatility</i> to calculate Exponentially\n",
    "Weighted Moving Average model to gauge market volatility:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613639db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ewma_volatility(df, span=30):\n",
    "    \"\"\"\n",
    "    Calculate the Exponentially Weighted Moving Average (EWMA) volatility for a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The input data frame containing a 'log_return' column.\n",
    "    span : int, optional (default=30)\n",
    "        The span for the EWMA, which determines the decay factor (lambda).\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    pandas.DataFrame\n",
    "        The input DataFrame with an additional 'EWMA_volatility' column containing the calculated volatility.\n",
    "    \"\"\"\n",
    "    # Calculate the squared log returns\n",
    "    squared_log_returns = df['log_return'] ** 2\n",
    "\n",
    "    # Calculate the EWMA of the squared log returns\n",
    "    # 'span' corresponds to the (2 / (1 + lambda)) in the decay formula, where lambda is the decay factor\n",
    "    ewma_variance = squared_log_returns.ewm(span=span).mean()\n",
    "\n",
    "    # The standard deviation (volatility) is the square root of the variance\n",
    "    ewma_volatility = np.sqrt(ewma_variance)\n",
    "\n",
    "    # Create EWMA volatility column\n",
    "    df[\"EWMA_volatility\"] = ewma_volatility\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e3c729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for exchange_pair in clean_data.keys():\n",
    "    for freq in clean_data[exchange_pair].keys():\n",
    "        # Calculate EWMA volatility for each frequency data\n",
    "        clean_data[exchange_pair][freq] = calculate_ewma_volatility(clean_data[exchange_pair][freq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9430c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_return_and_ewma_plot(df):\n",
    "    \"\"\"\n",
    "    Create plot traces for EWMA volatility and log returns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing 'EWMA_volatility' and 'log_return' columns.\n",
    "\n",
    "    Returns:\n",
    "    list: List of plotly graph objects for EWMA volatility and log return.\n",
    "    list: List of titles for the subplots.\n",
    "    \"\"\"\n",
    "    # Create a scatter plot for EWMA volatility\n",
    "    trace1 = go.Scatter(\n",
    "        x=df.index.values,  # x-axis values: datetime index\n",
    "        y=df['EWMA_volatility'].values,  # y-axis values: EWMA volatility\n",
    "        marker=dict(color=\"blue\"),  # marker color\n",
    "        name='ewma_volatility'  # legend name\n",
    "    )\n",
    "    \n",
    "    # Create a scatter plot for log return\n",
    "    trace2 = go.Scatter(\n",
    "        x=df.index.values,  # x-axis values: datetime index\n",
    "        y=df['log_return'].values,  # y-axis values: log return\n",
    "        marker=dict(color=\"red\"),  # marker color\n",
    "        name='log_return'  # legend name\n",
    "    )\n",
    "\n",
    "    # Return the list of traces and their titles\n",
    "    return [trace1, trace2], ['ewma_volatility', 'log_return']\n",
    "\n",
    "def plot_log_return_and_ewma(df_exchange_pair):\n",
    "    \"\"\"\n",
    "    Plot EWMA volatility and log returns for different frequency data.\n",
    "\n",
    "    Parameters:\n",
    "    df_exchange_pair (dict): Dictionary containing dataframes for different frequencies.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Iterate over each frequency in the dictionary\n",
    "    for freq in df_exchange_pair.keys():\n",
    "        # Get the price series data for the current frequency\n",
    "        price_series = df_exchange_pair[freq]\n",
    "        \n",
    "        # Create plot traces and titles\n",
    "        traces, titles = create_log_return_and_ewma_plot(price_series)\n",
    "\n",
    "        # Create subplots, one for each trace\n",
    "        fig = subplots.make_subplots(rows=len(traces), cols=1, subplot_titles=titles)\n",
    "\n",
    "        # Append each trace to the respective subplot\n",
    "        for i, trace in enumerate(traces):\n",
    "            fig.append_trace(trace, row=i + 1, col=1)\n",
    "\n",
    "        # Update layout of the figure\n",
    "        fig.update_layout(height=600, title_text=f\"time-frame-interval = {freq}\")\n",
    "        \n",
    "        # Display the plot\n",
    "        iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08adee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_log_return_and_ewma(clean_data[\"usdttmn-nobitex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28107e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_log_return_and_ewma(clean_data[\"usdttmn-wallex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defada51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_log_return_and_ewma(clean_data[\"usdttmn-calculated\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52909bf",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    <b>Volatility clustering</b> is a phenomenon in financial time series where periods of high volatility tend to be followed by high volatility and periods of low volatility tend to be followed by low volatility. This characteristic indicates that volatility is not constant over time and can be autocorrelated.\n",
    "</font></p>\n",
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    Now we cluster our volatility in 3 clusters:\n",
    "    <ol>\n",
    "        <li><b>High Volatility Clusters</b> : These periods often correspond to market stress or major economic announcements. For traders and risk managers, these clusters signal a need for increased caution, potential deleveraging, or the implementation of hedging strategies to protect against adverse movements.</li>\n",
    "        <li><b>Low Volatility Clusters</b> : Typically indicate periods of market calm but can also precede significant market moves (as per the \"calm before the storm\" scenario). Strategic decisions here might involve increasing exposure to capture potential moves or taking advantage of low option premiums when constructing hedging strategies.</li>\n",
    "        <li><b>Transition Phases Volatility Clusters</b> : The periods transitioning between high and low volatility are particularly critical as they might indicate a change in market regime. Detecting these changes early can be pivotal for adjusting portfolios in anticipation of increased risk or opportunity.</li>\n",
    "    </ol>\n",
    "</font></p>\n",
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    At last we mention implications for risk management and trading strategies:\n",
    "    <ol>\n",
    "        <li><b>Dynamic Position Sizing</b> : Adjusting trade sizes based on the prevailing volatility regime. Larger positions might be justified during low volatility regimes, whereas reducing exposure during high volatility can mitigate risk.</li>\n",
    "        <li><b>Stop-Loss Adjustments</b> : Volatility clustering can inform the placement of stop-loss orders. Wider stops might be necessary during high volatility regimes to avoid being stopped out prematurely.</li>\n",
    "        <li><b>Portfolio Diversification</b> : Understanding volatility patterns can guide the diversification strategies. During high volatility regimes, adding non-correlated assets or assets that thrive in volatile markets (like gold or certain types of options) can improve portfolio resilience.</li>\n",
    "        <li><b>Strategic Entry/Exit Timing</b> : Identifying clusters can help in timing market entry and exit, optimizing trade execution around expected volatility changes.</li>\n",
    "        <li><b>Stress Testing</b> : Using historical volatility clusters to model potential future scenarios and stress test portfolios against extreme events.</li>\n",
    "    </ol>\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9471763",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">3. Statistical Summaries:</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508fd89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_statistical_metrics(series, name):\n",
    "    \"\"\"\n",
    "    Calculate statistical metrics for a given series.\n",
    "\n",
    "    Parameters:\n",
    "    series (pd.Series): The data series for which to calculate metrics.\n",
    "    name (str): The name to assign to the column in the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing mean, standard deviation, skewness, and kurtosis of the series.\n",
    "    \"\"\"\n",
    "    # Calculate statistical metrics\n",
    "    statistical_metrics = {\n",
    "        \"mean\": series.mean(),\n",
    "        \"std\": series.std(),\n",
    "        \"skewness\": skew(series),\n",
    "        \"kurtosis\": kurtosis(series)\n",
    "    }\n",
    "    \n",
    "    # Convert the dictionary of metrics to a DataFrame\n",
    "    return pd.DataFrame.from_dict(statistical_metrics, orient='index', columns=[name])\n",
    "\n",
    "def find_statistical_metrics_ewma_and_log_return(df_exchange_pair):\n",
    "    \"\"\"\n",
    "    Calculate statistical metrics for EWMA volatility and log returns for different frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    df_exchange_pair (dict): Dictionary containing dataframes for different frequencies.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the statistical metrics for each frequency.\n",
    "    \"\"\"\n",
    "    all_infos = []  # List to store metrics for all frequencies\n",
    "    \n",
    "    # Iterate over each frequency in the dictionary\n",
    "    for freq in df_exchange_pair.keys():\n",
    "        # Get the price series data for the current frequency\n",
    "        price_series = df_exchange_pair[freq]\n",
    "        \n",
    "        # Calculate statistical metrics for log return and EWMA volatility\n",
    "        log_return_info = find_statistical_metrics(price_series['log_return'].dropna(), \"log_return\")\n",
    "        ewma_volatility_info = find_statistical_metrics(price_series['EWMA_volatility'].dropna(), \"EWMA_volatility\")\n",
    "        \n",
    "        # Append the combined metrics to the list\n",
    "        all_infos.append(pd.concat([log_return_info, ewma_volatility_info], axis=1))\n",
    "        \n",
    "    # Concatenate all metrics DataFrames into a single DataFrame\n",
    "    return pd.concat(all_infos, axis=1, keys=list(df_exchange_pair.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7a948",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_statistical_metrics_ewma_and_log_return(clean_data[\"usdttmn-wallex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fcd811",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_statistical_metrics_ewma_and_log_return(clean_data[\"usdttmn-nobitex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05573ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_statistical_metrics_ewma_and_log_return(clean_data[\"usdttmn-calculated\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da085827",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">4. Graphical Normality Tests:</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986307b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def qqplot_all_frequencies(df_exchange_pair, col):\n",
    "    \"\"\"\n",
    "    Generate QQ plots for a specified column across different frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    df_exchange_pair (dict): Dictionary containing dataframes for different frequencies.\n",
    "    col (str): The column for which to generate the QQ plots.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    num_frequencies = len(df_exchange_pair.keys())\n",
    "    num_cols = 2  # Number of columns in the subplot grid\n",
    "    num_rows = (num_frequencies + 1) // num_cols  # Number of rows in the subplot grid\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(10, 10))\n",
    "    ax = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "    fig.suptitle('Quantile-Quantile Plot')  # Set the super title for the figure\n",
    "    \n",
    "    # Iterate over each frequency and create a QQ plot\n",
    "    for i, freq in enumerate(df_exchange_pair.keys()):\n",
    "        qqplot(df_exchange_pair[freq][col].dropna(), fit=True, line='q', ax=ax[i])\n",
    "        ax[i].set_title(f\"time-frame-interval = {freq}\")  # Set title for each subplot\n",
    "        ax[i].set_xticks(np.arange(-5, 6, 1))  # Set x-axis ticks\n",
    "        ax[i].grid(True)  # Enable grid for each subplot\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for j in range(i + 1, len(ax)):\n",
    "        fig.delaxes(ax[j])\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to make room for the title\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3738c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot_all_frequencies(clean_data[\"usdttmn-wallex\"],'log_return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ff114",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot_all_frequencies(clean_data[\"usdttmn-nobitex\"],'log_return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3a611",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qqplot_all_frequencies(clean_data[\"usdttmn-calculated\"],'log_return')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c5cd89",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   After plotting <b>QQ plot</b> for log returns of existed resampled data as we see, <i>5-minute</i> data has the most similarity to normality distribution and <i>1440-minute</i> data has the least similarity.\n",
    "</font></p>\n",
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    We should mention that general form of <b>QQ plot</b> for log returns is <b>heavy tailed</b> format meaning that <u>compared to the normal distribution there is much more data located at the extremes of the distribution and less data in the center of the distribution</u> like this:\n",
    "    <img src=\"./images/fat_tail.png\" alt=\"HTML5 Icon\" style=\"width:300px;height:300px;\">\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e223823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def qqplot_data(series):\n",
    "    \n",
    "#     qqplot_data = probplot(series)\n",
    "#     x = np.array([qqplot_data[0][0][0], qqplot_data[0][0][-1]])\n",
    "    \n",
    "# #     fig = go.Figure()\n",
    "\n",
    "#     trace_1 = go.Scatter(\n",
    "#         x=qqplot_data[0][0],\n",
    "#         y=qqplot_data[0][1],\n",
    "#         mode='markers',\n",
    "#         marker=dict(color='#1f77b4')        \n",
    "#     )\n",
    "\n",
    "#     trace_2 = go.Scatter(\n",
    "#         x=x,\n",
    "#         y=qqplot_data[1][1]+x*qqplot_data[1][0],\n",
    "#         mode='lines',\n",
    "#         marker=dict(color='#ff4040')  \n",
    "#     )\n",
    "    \n",
    "#     return [trace_1, trace_2]\n",
    "\n",
    "# def qqplot_all_frequencies(df_exchange_pair, col):\n",
    "    \n",
    "#     figures = []\n",
    "#     subplot_titles = []\n",
    "    \n",
    "#     for freq in df_exchange_pair.keys():\n",
    "#         series = df_exchange_pair[freq][col].dropna()\n",
    "#         subplot_titles.append(f\"time-frame-interval = {freq}\")\n",
    "#         figures.append(qqplot_data(series))\n",
    "        \n",
    "#     fig = subplots.make_subplots(rows=len(figures), cols=1, subplot_titles=tuple(subplot_titles))\n",
    "\n",
    "#     for i, figure in enumerate(figures):\n",
    "#             for trace in figure:\n",
    "#                 fig.add_trace(trace, row=i+1, col=1)\n",
    "#                 fig.update_xaxes(title_text=\"Theoritical Quantities\", row=i+1, col=1)\n",
    "#                 fig.update_yaxes(title_text=\"Sample Quantities\", row=i+1, col=1)\n",
    "\n",
    "#     fig.update_layout(title_text='Quantile-Quantile Plot')\n",
    "#     iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qqplot_all_frequencies(clean_data[\"usdttmn-wallex\"],'log_return')\n",
    "# qqplot_all_frequencies(clean_data[\"usdttmn-nobitex\"],'log_return')\n",
    "# qqplot_all_frequencies(clean_data[\"usdttmn-calculated\"],'log_return')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db159502",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">5. Quantitative Normality Tests:</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee7ff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_shapiro_test(df_exchange_pair, col):\n",
    "    \"\"\"\n",
    "    Perform the Shapiro-Wilk test for normality on a specified column across different frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    df_exchange_pair (dict): Dictionary containing dataframes for different frequencies.\n",
    "    col (str): The column for which to perform the Shapiro-Wilk test.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the Shapiro-Wilk test statistic and p-value for each frequency.\n",
    "    \"\"\"\n",
    "    all_infos = []  # List to store test results for all frequencies\n",
    "    \n",
    "    # Iterate over each frequency in the dictionary\n",
    "    for freq in df_exchange_pair.keys():\n",
    "        # Perform the Shapiro-Wilk test on the specified column\n",
    "        statistic, pvalue = shapiro(df_exchange_pair[freq][col].dropna())\n",
    "        \n",
    "        # Create a DataFrame from the test results\n",
    "        test_results = pd.DataFrame.from_dict({\n",
    "            \"statistic\": statistic,\n",
    "            \"pvalue\": pvalue\n",
    "        }, orient='index', columns=[f\"Shapiro-Wilk Test\"])\n",
    "        \n",
    "        # Append the test results to the list\n",
    "        all_infos.append(test_results)\n",
    "    \n",
    "    # Concatenate all test results into a single DataFrame\n",
    "    return pd.concat(all_infos, axis=1, keys=list(df_exchange_pair.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee09a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_shapiro_test(clean_data[\"usdttmn-wallex\"],'log_return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a8d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_shapiro_test(clean_data[\"usdttmn-nobitex\"],'log_return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_shapiro_test(clean_data[\"usdttmn-calculated\"],'log_return')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39156bc7",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    we should consider that for NumOfSamples > 5000 the W test statistic is accurate, but the p-value may not be.\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c999642",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">6. Importance of Normality:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359bec1",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    Now we name the important factors explained significance of a normal distribution in Financial Modeling and Risk Assessment:\n",
    "    <ol>\n",
    "        <li><b>Predictable Properties</b> : The normal distribution is essential in financial modeling because it has well-known properties that facilitate analytic solutions and statistical inference. It is symmetric around its mean, describing how the values of a variable are distributed. This makes it easier to calculate probabilities and quantiles.</li>\n",
    "        <li><b>Risk Measures</b> : In risk management, measures such as Value at Risk (VaR) and Expected Shortfall (ES) often rely on normally distributed return assumptions for simplicity and calculability. These measures are pivotal in determining the potential loss in value of assets or portfolios.</li>\n",
    "        <li><b>Central Limit Theorem</b> : The significance of the normal distribution is further underscored by the Central Limit Theorem (CLT), which states that the sum (or average) of a large number of independent, identically distributed variables tends toward a normal distribution, regardless of the original distribution of the variables. This property is widely used to justify the use of normal distributions in various models.</li>\n",
    "    </ol>\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b94e05f",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    Finally, we discuss about Challenges of Non-Normality:\n",
    "    <ol>\n",
    "        <li><b>Fat Tails and Extreme Events</b> : Real-world financial data often exhibit fat tails, meaning they have a higher probability of extreme values than predicted by a normal distribution. This non-normality can significantly alter risk assessments by underestimating the likelihood and impact of extreme market movements.</li>\n",
    "        <li><b>Skewness</b> : Financial returns can be skewed rather than symmetrically distributed, causing models that assume normal distribution to misestimate risk. Positive or negative skewness implies that returns are more likely to be greater or less than what a normal distribution suggests, impacting decision-making and risk strategies.</li>\n",
    "        <li><b>Volatility Clustering</b> : Financial markets often exhibit periods of high and low volatility clustering. Models based on normal distribution assumptions might fail to capture this dynamic characteristic, possibly leading to underestimating risk in volatile periods and overestimating it during stable periods. This inconsistency can result in ineffective risk management and strategy development.</li>\n",
    "        <li><b>Incorrect Parameter Estimation</b> : Non-normality in data affects parameter estimation procedures, leading to biased or inconsistent estimates of mean, variance, and correlations. This could result in inaccurate risk evaluations and affect the reliability of stress tests and scenario analyses.</li>\n",
    "    </ol>\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5b1c85",
   "metadata": {},
   "source": [
    "<h2><font face=\"gotham\" color=\"#3b5996\">2.2 Autocorrelation and Stationarity Analysis</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd70e045",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">1. ACF and PACF Plots:</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corr_plot(series, mode, alpha):\n",
    "    \"\"\"\n",
    "    Create correlation plot traces for ACF or PACF.\n",
    "\n",
    "    Parameters:\n",
    "    series (pd.Series): The time series data.\n",
    "    mode (str): Mode of correlation ('acf' or 'pacf').\n",
    "    alpha (float): Significance level for confidence intervals.\n",
    "\n",
    "    Returns:\n",
    "    list: List of plotly graph objects for the correlation plot.\n",
    "    \"\"\"\n",
    "    # Calculate correlation array and confidence intervals based on mode\n",
    "    match mode:\n",
    "        case \"acf\":\n",
    "            corr_array = acf(series.dropna(), alpha=alpha)\n",
    "        case \"pacf\":\n",
    "            corr_array = pacf(series.dropna(), alpha=alpha)\n",
    "    \n",
    "    # Calculate the lower and upper confidence intervals\n",
    "    lower_y = corr_array[1][:, 0] - corr_array[0]\n",
    "    upper_y = corr_array[1][:, 1] - corr_array[0]\n",
    "    \n",
    "    # Create traces for the correlation plot\n",
    "    traces = [go.Scatter(x=(x, x), y=(0, corr_array[0][x]), mode='lines', line_color='#3f3f3f') for x in range(len(corr_array[0]))]\n",
    "    traces.append(go.Scatter(x=np.arange(len(corr_array[0])), y=corr_array[0], mode='markers', marker_color='#1f77b4', marker_size=12))\n",
    "    traces.append(go.Scatter(x=np.arange(len(corr_array[0])), y=upper_y, mode='lines', line_color='rgba(255, 255, 255, 0)'))\n",
    "    traces.append(go.Scatter(x=np.arange(len(corr_array[0])), y=lower_y, mode='lines', fillcolor='rgba(32, 146, 230, 0.3)',\n",
    "                             fill='tonexty', line_color='rgba(255, 255, 255, 0)'))\n",
    "    return traces\n",
    "\n",
    "def create_corr_plot_all_frequencies(df_exchange_pair, col, mode, alpha):\n",
    "    \"\"\"\n",
    "    Create correlation plots for a specified column across different frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    df_exchange_pair (dict): Dictionary containing dataframes for different frequencies.\n",
    "    col (str): The column for which to generate the correlation plots.\n",
    "    mode (str): Mode of correlation ('acf' or 'pacf').\n",
    "    alpha (float): Significance level for confidence intervals.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Set plot title based on mode\n",
    "    title = \"Autocorrelation (ACF)\" if mode == \"acf\" else \"Partial Autocorrelation (PACF)\"\n",
    "    \n",
    "    # Generate subplot titles\n",
    "    titles = [f\"time-frame-interval = {freq}\" for freq in df_exchange_pair.keys()]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = subplots.make_subplots(rows=len(df_exchange_pair.keys()), cols=1, subplot_titles=tuple(titles))\n",
    "    \n",
    "    # Create and add traces for each frequency\n",
    "    for i, freq in enumerate(df_exchange_pair.keys()):\n",
    "        series = df_exchange_pair[freq][col]\n",
    "        corr_traces = create_corr_plot(series, mode, alpha)\n",
    "        \n",
    "        for trace in corr_traces:\n",
    "            fig.add_trace(trace, row=i+1, col=1)\n",
    "            fig.update_xaxes(title_text=\"lag\", row=i+1, col=1)\n",
    "            fig.update_yaxes(title_text=mode, row=i+1, col=1)\n",
    "            fig.update_traces(showlegend=False)\n",
    "            fig.update_xaxes(range=[-1, 42])\n",
    "            fig.update_yaxes(zerolinecolor='#000000')\n",
    "    \n",
    "    # Update layout and display plot\n",
    "    fig.update_layout(title_text=title, height=1000)\n",
    "    iplot(fig)\n",
    "\n",
    "def create_acf_and_pacf_plot(df_exchange_pair, col, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Create both ACF and PACF plots for a specified column across different frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    df_exchange_pair (dict): Dictionary containing dataframes for different frequencies.\n",
    "    col (str): The column for which to generate the correlation plots.\n",
    "    alpha (float): Significance level for confidence intervals.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    create_corr_plot_all_frequencies(df_exchange_pair, col, 'acf', alpha)\n",
    "    create_corr_plot_all_frequencies(df_exchange_pair, col, 'pacf', alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0fc0b1",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "   Fisrt, we set significance level for confidence intervals or $\\alpha$ to 0.05.\n",
    "</font></p>\n",
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    As the next step, we plot <b>ACF</b> and <b>PACF</b> of log retrun column for each frequency separately:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c5b476",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_acf_and_pacf_plot(clean_data[\"usdttmn-wallex\"],'log_return', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd1fa36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_acf_and_pacf_plot(clean_data[\"usdttmn-nobitex\"],'log_return', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0996c1b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_acf_and_pacf_plot(clean_data[\"usdttmn-calculated\"],'log_return', alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e462d",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    According to <b>ACF</b> and <b>PACF</b> above plots, we can see that for <u>1440-min resampled data</u> there is <u>no correlation between original series and lagged series</u>, but in <u>other frequencies resampled data</u>, we obviosly see <u>a significant corrolation with fisrt lag</u>.\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700ada30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_acf_and_pacf_plot(clean_data[\"usdttmn-wallex\"],'EWMA_volatility', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda97b9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_acf_and_pacf_plot(clean_data[\"usdttmn-nobitex\"],'EWMA_volatility', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09986b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_acf_and_pacf_plot(clean_data[\"usdttmn-calculated\"],'EWMA_volatility', alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99662efb",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    According to <b>ACF</b> plot of <i>EWMA_volatility</i> column as seen above, we can see that for all resampled data <u>there is a significant correlation with lagged data and the scatter decays slowly indicating high persistence in volatility (volatility clustering)</u>. And by looking at <b>PACF</b> plot, we can identify at which lag the autocorrelation effect diminishes. <u>For most of them first lag and second lag are correlated significantly except 1440min frequency resampled data that include only first lag</u>.\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8070de06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_acf_and_pacf_plot(clean_data[\"usdttmn-wallex\"],'price', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e9a0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_acf_and_pacf_plot(clean_data[\"usdttmn-nobitex\"],'price', alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f13e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_acf_and_pacf_plot(clean_data[\"usdttmn-calculated\"],'price', alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7171fb",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    <b>ACF</b> and <b>PACF</b> plot for <i>price series</i> are pretty similar <u>except speed of decaying acf plot so there's more lagged data which are correlated</u>.\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0d2861",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">2. Stationarity Testing:</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series):\n",
    "    \"\"\"\n",
    "    Perform the Augmented Dickey-Fuller (ADF) test for stationarity on a given series.\n",
    "\n",
    "    Parameters:\n",
    "    series (pd.Series): The time series data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the ADF test statistic, p-value, number of lags used, \n",
    "                  number of observations used, and critical values.\n",
    "    \"\"\"\n",
    "    indices = [\"Test Statistic\", \"p-value\", \"# of Lags Used\", \"# of Observations Used\"]\n",
    "    \n",
    "    # Perform ADF test\n",
    "    adf_results_all = adfuller(series.dropna())\n",
    "    results = pd.Series(adf_results_all[0:4], index=indices)\n",
    "    \n",
    "    # Add critical values to the results\n",
    "    for key, value in adf_results_all[4].items():\n",
    "        results[f'Critical Value ({key})'] = value\n",
    "    \n",
    "    # Convert the results to a DataFrame\n",
    "    results = pd.DataFrame(results, columns=['statistics'])\n",
    "    return results\n",
    "\n",
    "def adf_test_for_all_frequencies(df_exchange_pair, col):\n",
    "    \"\"\"\n",
    "    Perform the Augmented Dickey-Fuller (ADF) test for stationarity on a specified column \n",
    "    across different frequencies.\n",
    "\n",
    "    Parameters:\n",
    "    df_exchange_pair (dict): Dictionary containing dataframes for different frequencies.\n",
    "    col (str): The column for which to perform the ADF test.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the ADF test results for each frequency.\n",
    "    \"\"\"\n",
    "    all_infos = []  # List to store test results for all frequencies\n",
    "    \n",
    "    # Iterate over each frequency in the dictionary\n",
    "    for freq in df_exchange_pair.keys():\n",
    "        series = df_exchange_pair[freq][col]\n",
    "        adf_result = adf_test(series)\n",
    "        all_infos.append(adf_result)\n",
    "    \n",
    "    # Concatenate all test results into a single DataFrame\n",
    "    return pd.concat(all_infos, axis=1, keys=list(df_exchange_pair.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d4832e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adf_test_for_all_frequencies(clean_data[\"usdttmn-wallex\"], 'log_return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adcdd3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adf_test_for_all_frequencies(clean_data[\"usdttmn-nobitex\"], 'log_return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb721e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adf_test_for_all_frequencies(clean_data[\"usdttmn-calculated\"], 'log_return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396ea0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adf_test_for_all_frequencies(clean_data[\"usdttmn-wallex\"], 'EWMA_volatility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4173bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adf_test_for_all_frequencies(clean_data[\"usdttmn-nobitex\"], 'EWMA_volatility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f622789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adf_test_for_all_frequencies(clean_data[\"usdttmn-calculated\"], 'EWMA_volatility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938920e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adf_test_for_all_frequencies(clean_data[\"usdttmn-wallex\"], 'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07089013",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adf_test_for_all_frequencies(clean_data[\"usdttmn-nobitex\"], 'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a90ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adf_test_for_all_frequencies(clean_data[\"usdttmn-calculated\"], 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1bb26",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">3. Interplay of Non-Stationarity and Autocorrelation:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bbe244",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    Non-stationarity and autocorrelation are fundamental characteristics in financial time series, and their interrelation is pivotal for effective modeling. First, we discuss about these concepts:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b142d8f9",
   "metadata": {},
   "source": [
    "<table style=\"border: 1px solid black; border-collapse: collapse;\"\n",
    ">\n",
    "  <tr style=\"border: 1px solid black; border-collapse: collapse;\">\n",
    "    <th></th>\n",
    "    <th style=\"font-size:17px; text-align:center;\"><font face=\"gotham\">Non-Stationarity</font></th>\n",
    "  <tr style=\"border: 1px solid black; border-collapse: collapse;\">\n",
    "    <td style=\"font-size:17px; text-align:center;\"><font face=\"gotham\"><b>Definition</b></font></td>\n",
    "    <td style=\"font-size:17px; text-align:left;\"><font face=\"gotham\">Non-stationarity refers to time series data whose statistical properties like mean, variance, and covariance are not constant over time. Financial markets often display trends, volatility clustering, and structural breaks, indicative of non-stationary behavior.</font></td>\n",
    "  </tr>\n",
    "  <tr style=\"border: 1px solid black; border-collapse: collapse;\">\n",
    "    <td style=\"font-size:17px; text-align:center;\"><font face=\"gotham\"><b>Types</b></font></td>\n",
    "    <td style=\"font-size:17px; text-align:left;\"><font face=\"gotham\">\n",
    "        <lo>\n",
    "            <li><b>Trend Non-Stationarity</b>: Persistent increase or decrease over time, often removing the trend can achieve stationarity.</li>\n",
    "            <li><b>Variance Non-Stationarity (Heteroscedasticity)</b>: Changing variance over time, often seen in periods of high and low volatility.</li>\n",
    "        </lo>\n",
    "    </font></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<table style=\"border: 1px solid black; border-collapse: collapse;\">\n",
    "  <tr style=\"border: 1px solid black; border-collapse: collapse;\">\n",
    "    <th></th>\n",
    "    <th style=\"font-size:17px; text-align:center;\"><font face=\"gotham\">Autocorrelation</font></th>\n",
    "  <tr style=\"border: 1px solid black; border-collapse: collapse;\">\n",
    "    <td style=\"font-size:17px; text-align:center;\"><font face=\"gotham\"><b>Definition</b></font></td>\n",
    "    <td style=\"font-size:17px; text-align:left;\"><font face=\"gotham\">Autocorrelation measures the correlation of a time series with its own past values. Positive autocorrelation indicates that high values follow high values, and low values follow low values.</font></td>\n",
    "  </tr>\n",
    "  <tr style=\"border: 1px solid black; border-collapse: collapse;\">\n",
    "    <td style=\"font-size:17px; text-align:center;\"><font face=\"gotham\"><b>Implications</b></font></td>\n",
    "    <td style=\"font-size:17px; text-align:left;\"><font face=\"gotham\">\n",
    "        Autocorrelation in financial time series data suggests that past values can provide information about future values, which is crucial for predictive modeling.\n",
    "    </font></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c5266",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    Now we explain the <u>interrelation of Non-Stationarity on Autocorrelation</u>:\n",
    "    <ol>\n",
    "        <li><b>Implications of Non-Stationarity on Autocorrelation</b> : <ul><li>Non-stationary data often exhibit patterns such as trends or periodic fluctuations, which introduce autocorrelation. For instance, a trending financial time series will show strong positive autocorrelation.</li><li>High and autocorrelated volatility can indicate autocorrelated residuals, as seen in many financial return series.</li></ul>\n",
    "        </li>\n",
    "        <li><b>Implications for Effective Modeling</b> : \n",
    "        <ul><li><b>Modeling Pitfalls</b>: Using non-stationary data directly in models that assume stationarity can lead to spurious results, unreliable predictions, and faulty inferences.</li>\n",
    "            <li><b>Modeling Pitfalls</b>: Using non-stationary data directly in models that assume stationarity can lead to spurious results, unreliable predictions, and faulty inferences.</li></ul>\n",
    "        </li>\n",
    "        <li><b>Transformations for Stationarity</b> : \n",
    "            <ul>\n",
    "                <li><b>Differencing</b>: Subtracting consecutive observations to eliminate trends.\n",
    "        </li>\n",
    "        <li><b>Log Transformation</b>: Stabilizing variance.\n",
    "        </li>\n",
    "        <li><b>Detrending</b>: Removing deterministic trends to achieve stationarity.\n",
    "        </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ol>\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffe789d",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    Finally, Now we discuss about the <u>Implications for Predictive Modeling and Strategy Development</u>:\n",
    "    <ol>\n",
    "        <li><b>Predictive Modeling</b> : \n",
    "        <ul><li><b>Stationary Data Requirement</b>: Predictive models like ARMA (AutoRegressive Moving Average) and ARIMA (AutoRegressive Integrated Moving Average) require data to be stationary.</li>\n",
    "            <li><b>Transformations</b>: Techniques like differencing the series or applying transformations such as logarithms can convert non-stationary series into stationary ones suitable for these models.</li>\n",
    "        </ul>\n",
    "        <li><b>Strategy Development</b> : \n",
    "        <ul><li><b>Risk Assessment</b>: Understanding autocorrelation and non-stationarity helps in better risk assessment, as these factors often precede significant market shifts.</li>\n",
    "            <li><b>Volatility Clustering</b>: Recognizing and modeling periods of high and low volatility (volatility clustering) is crucial for developing strategies that are sensitive to changing market conditions.</li>\n",
    "        </ul>\n",
    "    </ol>\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d388e834",
   "metadata": {},
   "source": [
    "<h2><font face=\"gotham\" color=\"#3b5996\">2.3 Inter-Market Analysis</font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e147a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorr(datax, datay, window=0, lag=0):\n",
    "    \"\"\"\n",
    "    Computes time-lagged cross correlation where 'datay' is shifted by 'lag'.\n",
    "    Shifted data filled with NaNs.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    datax, datay : pd.Series\n",
    "        Pandas Series objects of equal length.\n",
    "    window : int, optional\n",
    "        Size of the rolling window (default is 0).\n",
    "    lag : int, optional\n",
    "        Lag value for time-lagged cross correlation (default is 0).\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    pd.Series\n",
    "        Time-lagged cross correlation.\n",
    "    \"\"\"\n",
    "    return datax.rolling(window=window).corr(datay.shift(lag))\n",
    "\n",
    "# Dictionary mapping time frames to the number of data points in a month\n",
    "month_in_time_frames = {\n",
    "    \"5min\": 30 * 24 * 12,\n",
    "    \"20min\": 30 * 24 * 3,\n",
    "    \"60min\": 30 * 24 * 1,\n",
    "    \"1440min\": 30 * 1\n",
    "}\n",
    "\n",
    "def create_cross_corr_plot(data, scatter_name):\n",
    "    \"\"\"\n",
    "    Create a scatter plot for cross correlation data.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    data : pd.Series\n",
    "        Time-lagged cross correlation data.\n",
    "    scatter_name : str\n",
    "        Name for the scatter plot.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    go.Scatter\n",
    "        Plotly scatter plot object.\n",
    "    \"\"\"\n",
    "    return go.Scatter(\n",
    "        x=data.index,\n",
    "        y=data.values,\n",
    "        name=scatter_name\n",
    "    )\n",
    "\n",
    "def plot_cross_corr_two_pair(df1_exchange_pair, df2_exchange_pair, col, window_table, lag=0):\n",
    "    \"\"\"\n",
    "    Plot cross correlation between two pairs of dataframes.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df1_exchange_pair : dict\n",
    "        Dictionary containing dataframes for the first pair.\n",
    "    df2_exchange_pair : dict\n",
    "        Dictionary containing dataframes for the second pair.\n",
    "    col : str\n",
    "        Column name for which to compute cross correlation.\n",
    "    window_table : dict\n",
    "        Dictionary containing window sizes for different frequencies.\n",
    "    lag : int, optional\n",
    "        Lag value for time-lagged cross correlation (default is 0).\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Iterate over each frequency in the dataframes\n",
    "    for freq in df1_exchange_pair.keys():\n",
    "        # Compute time-lagged cross correlation\n",
    "        cross_corr_data = crosscorr(\n",
    "            df1_exchange_pair[freq][col],\n",
    "            df2_exchange_pair[freq][col],\n",
    "            window=window_table[freq],\n",
    "            lag=lag\n",
    "        )\n",
    "        trace = create_cross_corr_plot(cross_corr_data, f\"time-frame-interval = {freq}\")\n",
    "        fig.add_trace(trace)\n",
    "    \n",
    "    # Update layout and display plot\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Cross-correlation with lag = {lag}\",\n",
    "        yaxis_range=[-1, 1]\n",
    "    )\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ce9380",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">1. Synchronous Correlations:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b06e5e8",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    In this part we calculate and plot the immediate correlation of log returns across pairs of\n",
    "    <i>USDT-TMN</i> prices <b>(lag 0)</b> using a <b>rolling window of one month</b>:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bca40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_corr_two_pair(clean_data[\"usdttmn-nobitex\"], clean_data[\"usdttmn-wallex\"], 'log_return', month_in_time_frames, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb4fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_corr_two_pair(clean_data[\"usdttmn-nobitex\"], clean_data[\"usdttmn-calculated\"], 'log_return', month_in_time_frames, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d177fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_corr_two_pair(clean_data[\"usdttmn-calculated\"], clean_data[\"usdttmn-wallex\"], 'log_return', month_in_time_frames, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67896950",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">2. Lagged Correlations:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3d383",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    As the next step, we calculate and plot the correlations where one set of returns is lagged by one time\n",
    "interval <b>(lag 1)</b> using a <b>rolling window of one month</b>:\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42753233",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_corr_two_pair(clean_data[\"usdttmn-nobitex\"], clean_data[\"usdttmn-wallex\"], 'log_return', month_in_time_frames, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f060ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_corr_two_pair(clean_data[\"usdttmn-wallex\"], clean_data[\"usdttmn-nobitex\"], 'log_return', month_in_time_frames, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_corr_two_pair(clean_data[\"usdttmn-nobitex\"], clean_data[\"usdttmn-calculated\"], 'log_return', month_in_time_frames, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da515d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_corr_two_pair(clean_data[\"usdttmn-calculated\"], clean_data[\"usdttmn-nobitex\"], 'log_return', month_in_time_frames, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8420c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_corr_two_pair(clean_data[\"usdttmn-calculated\"], clean_data[\"usdttmn-wallex\"], 'log_return', month_in_time_frames, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00157dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cross_corr_two_pair(clean_data[\"usdttmn-calculated\"], clean_data[\"usdttmn-nobitex\"], 'log_return', month_in_time_frames, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dbc0e9",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">3. Application to Strategy:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f023718",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    To utilize the insights from both synchronous and lagged correlations in trading strategies and risk management, we have four steps:\n",
    "    <ol>\n",
    "        <li><b>Identify trading opportunities</b>:\n",
    "            <ul>\n",
    "                <li><b>Synchronous correlations</b>: High synchronous correlations between pairs suggest that the markets move together. Low or negative synchronous correlations indicate potential for diversification. <u>As we excpect, In this analysis for 1440-min data, we see a high correlation between USDT-TMN exchange pairs, and for higher frequency like 5min this correlation decrease significantly.</u>\n",
    "                </li>\n",
    "                <li><b>Lagged correlations</b>: If one market consistently leads another, this can be exploited for predictive trading strategies.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><b>Arbitrage opportunities</b>: We can use lagged correlations to identify when one market is likely to move based on the movements of another market. If market A leads market B, then significant movements in A can predict future movements in B, allowing for arbitrage.\n",
    "        </li>\n",
    "        <li><b>Risk management</b>: We can incorporate correlation analysis into portfolio construction to diversify risk. For instance, pairs with low or negative correlations can be combined to reduce overall portfolio volatility.\n",
    "        </li>\n",
    "        <li><b>Strategic decision-making</b>: We can use the insights to inform entry and exit points for trades. For example, if a lagged correlation indicates that market B will move after market A, then a significant move in A can signal an entry or exit in B.\n",
    "        </li>\n",
    "    </ol>\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd63a7b",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color: #3b5996; padding: 10px;border-radius: 25px;\"><center><font face=\"gotham\" color=\"white\">Task 3: Cointegration Analysis</font></center><h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c19b7",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">1. Cointegration Testing Methodology:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3fc7ce",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    First, we explain the selection between raw price or log-transformed price for using in cointegration tests:\n",
    "    <ol>\n",
    "        <li><b>Raw Price</b>: \n",
    "            Financial time series data, such as stock prices, exchange rates, and commodity prices, are typically non-stationary. This means that their statistical properties (like mean and variance) change over time, often exhibiting trends and volatility. Using raw prices directly can lead to misleading results in cointegration tests because the tests assume that the series are either individually integrated of the same order or stationary.</li>\n",
    "        <li><b>Log-Transformed Prices</b>: \n",
    "            Applying a logarithmic transformation to prices helps stabilize the variance and convert multiplicative relationships into additive ones. Log transformation is particularly useful when dealing with financial time series because it tends to mitigate heteroscedasticity (changing variance over time) and helps in achieving stationarity in returns. Log returns (differences of log prices) are often found to be more stationary than raw returns.</li>\n",
    "    </ol>\n",
    "</font></p>\n",
    "\n",
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    <u>So, we choose log-transformed prices</u>.\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cointegration_test(X, Y, return_mode=\"pvalue\"):\n",
    "    \"\"\"\n",
    "    Perform cointegration test between two time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, Y : pd.Series\n",
    "        Time series data to be tested.\n",
    "    return_mode : str, optional\n",
    "        Mode of returning results (default is \"pvalue\").\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float or tuple\n",
    "        Cointegration test results based on return_mode.\n",
    "    \"\"\"\n",
    "    # Take logarithm of the series\n",
    "    X = np.log(X)\n",
    "    # Add a constant term for the linear regression\n",
    "    X = sm.add_constant(X)\n",
    "    Y = np.log(Y)\n",
    "\n",
    "    # Fit the linear regression model\n",
    "    lr_model = sm.OLS(Y, X)\n",
    "    lr_model_fit = lr_model.fit()\n",
    "\n",
    "    # Get the regression coefficients\n",
    "    alpha, beta = lr_model_fit.params\n",
    "    # Perform the Augmented Dickey-Fuller test on the residuals\n",
    "    pvalue = adfuller(lr_model_fit.resid)[1]\n",
    "    \n",
    "    # Return results based on the specified mode\n",
    "    match return_mode:\n",
    "        case \"pvalue\":\n",
    "            return pvalue\n",
    "        case \"coefficients+pvalue\":\n",
    "            return alpha, beta, pvalue\n",
    "        case \"variables+resid\":\n",
    "            return X['const'], X['price'], Y, lr_model_fit.resid\n",
    "        case _:\n",
    "            raise ValueError(\"Invalid return_mode. Choose from 'pvalue', 'coefficients+pvalue', 'variables+resid'.\")\n",
    "\n",
    "\n",
    "def check_cointegration(series1, series2, cutoff=0.01):\n",
    "    \"\"\"\n",
    "    Check if two time series are cointegrated.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series1, series2 : pd.Series\n",
    "        Time series data to be tested.\n",
    "    cutoff : float, optional\n",
    "        Cutoff value for p-value in cointegration test (default is 0.01).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the series are cointegrated, False otherwise.\n",
    "    \"\"\"\n",
    "    # Perform the cointegration test and get the p-value\n",
    "    pvalue = cointegration_test(series1, series2, return_mode=\"pvalue\")\n",
    "    # Return True if p-value is below the cutoff, indicating cointegration\n",
    "    return pvalue < cutoff\n",
    "\n",
    "\n",
    "def check_cointegration_all_frequencies(df1_exchange_pair, df2_exchange_pair, col, cutoff=0.01):\n",
    "    \"\"\"\n",
    "    Check cointegration for all frequencies in the given dataframes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df1_exchange_pair, df2_exchange_pair : dict\n",
    "        Dictionaries containing dataframes for the two pairs of exchanges.\n",
    "    col : str\n",
    "        Column name for which to compute cointegration.\n",
    "    cutoff : float, optional\n",
    "        Cutoff value for p-value in cointegration test (default is 0.01).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe containing cointegration information for all frequencies.\n",
    "    \"\"\"\n",
    "    all_cointegration_infos = []\n",
    "    # Iterate over each frequency\n",
    "    for freq in df1_exchange_pair.keys():\n",
    "        # Extract the series for the given frequency\n",
    "        series1 = df1_exchange_pair[freq][col]\n",
    "        series2 = df2_exchange_pair[freq][col]\n",
    "        # Perform the cointegration test and check if the series are cointegrated\n",
    "        cointegration_info = {\n",
    "            \"pvalue\": cointegration_test(series1, series2, return_mode=\"pvalue\"),\n",
    "            \"is_cointegrated\": check_cointegration(series1, series2, cutoff)\n",
    "        }\n",
    "        # Print the p-value for the current frequency\n",
    "        print(f'For time-frame interval = {freq}, p-value = {cointegration_info[\"pvalue\"]}')\n",
    "        # Append the cointegration information to the list\n",
    "        all_cointegration_infos.append(pd.DataFrame.from_dict(cointegration_info, orient='index', columns=[col]))\n",
    "\n",
    "    # Concatenate all the cointegration information into a single dataframe\n",
    "    return pd.concat(all_cointegration_infos, axis=1, keys=df1_exchange_pair.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b0d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cointegration_all_frequencies(clean_data[\"usdttmn-nobitex\"], clean_data[\"usdttmn-calculated\"], 'price', cutoff=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3894f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cointegration_all_frequencies(clean_data[\"usdttmn-nobitex\"], clean_data[\"usdttmn-wallex\"], 'price', cutoff=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cointegration_all_frequencies(clean_data[\"usdttmn-wallex\"], clean_data[\"usdttmn-calculated\"], 'price', cutoff=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f5770",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    Now we mention criteria for test selection:\n",
    "    <ol>\n",
    "        <li><b>Stationarity Testing</b>: \n",
    "            Before testing for cointegration, we should ensure that the individual time series are non-stationary but become stationary after differencing. This is because cointegration assumes that the series are integrated of the same order (typically I(1)).<u> we see in the previous sections that price series are not stationary but their log-transformed difference(log return) are stationary.</u>                     </li>\n",
    "        <li><b>Order of Integration</b>: In last parts we verify that the time series are integrated of the same order, in this case I(1). Because if the series are integrated of different orders, cointegration tests are not appropriate.\n",
    "        </li>\n",
    "        <li><b>Cointegration Testing</b>: We should determine if a long-term equilibrium relationship exists between non-stationary time series.<u> Since the <i>pvalue</i> of Augmented Dickey-Fuller (ADF) test are all less than our cutoff so all pairs are cointegrated</u>\n",
    "        </li>\n",
    "    </ol>\n",
    "</font></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550776af",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">2. Dynamic Analysis of Cointegration Parameters:</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e23109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_series_to_monthly_segments(series):\n",
    "    \"\"\"\n",
    "    Splits a time series into monthly segments.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series : pd.Series\n",
    "        Time series data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary where keys are month names and values are corresponding dataframes.\n",
    "    \"\"\"\n",
    "    # Group by month and return as a dictionary with month names as keys\n",
    "    return {month.month_name(): df for month, df in series.groupby(pd.Grouper(freq='M'))}\n",
    "\n",
    "def create_cointegration_info_plot(monthly_segments1, monthly_segments2, cutoff):\n",
    "    \"\"\"\n",
    "    Create plots for cointegration information for each month.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    monthly_segments1, monthly_segments2 : dict\n",
    "        Dictionaries containing monthly segmented dataframes.\n",
    "    cutoff : float\n",
    "        Cutoff value for p-value in cointegration test.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of Plotly trace objects for plotting.\n",
    "    \"\"\"\n",
    "    # Initialize an array to store alpha, beta, p-value, and cutoff for each month\n",
    "    info_monthly = np.zeros((4, 12))\n",
    "    \n",
    "    # Iterate over each month and perform cointegration test\n",
    "    for i, month in enumerate(monthly_segments1.keys()):\n",
    "        series1 = monthly_segments1[month]\n",
    "        series2 = monthly_segments2[month]\n",
    "        alpha, beta, pvalue = cointegration_test(series1, series2, return_mode=\"coefficients+pvalue\")\n",
    "        info_monthly[0][i] = alpha\n",
    "        info_monthly[1][i] = beta\n",
    "        info_monthly[2][i] = pvalue\n",
    "        info_monthly[3][i] = cutoff\n",
    "    \n",
    "    # Create Plotly traces for alpha, beta, p-value, and cutoff\n",
    "    trace1 = go.Scatter(\n",
    "        x=list(monthly_segments1.keys()),\n",
    "        y=info_monthly[0],\n",
    "        marker=dict(color=\"blue\"),\n",
    "        mode=\"lines+markers\",\n",
    "        name='alpha'\n",
    "    )\n",
    "    trace2 = go.Scatter(\n",
    "        x=list(monthly_segments1.keys()),\n",
    "        y=info_monthly[1],\n",
    "        marker=dict(color=\"red\"),\n",
    "        mode=\"lines+markers\",\n",
    "        name='beta'\n",
    "    )\n",
    "    trace3 = go.Scatter(\n",
    "        x=list(monthly_segments1.keys()),\n",
    "        y=info_monthly[2],\n",
    "        marker=dict(color=\"green\"),\n",
    "        mode=\"lines+markers\",\n",
    "        name='pvalue'\n",
    "    )\n",
    "    trace4 = go.Scatter(\n",
    "        x=list(monthly_segments1.keys()),\n",
    "        y=info_monthly[3],\n",
    "        marker=dict(color=\"brown\"),\n",
    "        mode=\"lines\",\n",
    "        name='cutoff'\n",
    "    )\n",
    "    \n",
    "    return [trace1, trace2, trace3, trace4]\n",
    "\n",
    "def plot_cointegration_info_monthly(df1_exchange_pair, df2_exchange_pair, col, cutoff=0.01):\n",
    "    \"\"\"\n",
    "    Plot cointegration information for monthly segments across multiple time frames.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df1_exchange_pair, df2_exchange_pair : dict\n",
    "        Dictionaries containing dataframes for the two pairs of exchanges.\n",
    "    col : str\n",
    "        Column name for which to compute cointegration.\n",
    "    cutoff : float, optional\n",
    "        Cutoff value for p-value in cointegration test (default is 0.01).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    subplot_titles = []\n",
    "    # Create subplot titles\n",
    "    for freq in df1_exchange_pair.keys():\n",
    "        subplot_titles.extend([\n",
    "            f\"time-frame-interval={freq} alpha\", \n",
    "            f\"time-frame-interval={freq} beta\", \n",
    "            f\"time-frame-interval={freq} pvalue\"\n",
    "        ])\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = subplots.make_subplots(rows=len(df1_exchange_pair.keys()), cols=3, subplot_titles=subplot_titles)\n",
    "\n",
    "    # Iterate over each frequency in the dataframes\n",
    "    for i, freq in enumerate(df1_exchange_pair.keys()):\n",
    "        # Split the series into monthly segments\n",
    "        monthly_segments1 = split_series_to_monthly_segments(df1_exchange_pair[freq][col])\n",
    "        monthly_segments2 = split_series_to_monthly_segments(df2_exchange_pair[freq][col])\n",
    "        \n",
    "        # Create cointegration info plots\n",
    "        traces = create_cointegration_info_plot(monthly_segments1, monthly_segments2, cutoff)\n",
    "\n",
    "        # Add each trace to the subplot\n",
    "        for j, trace in enumerate(traces):\n",
    "            j = j-1 if j==3 else j  # Adjust column index for cutoff trace\n",
    "            fig.add_trace(trace, row=i+1, col=j+1)\n",
    "            fig.update_xaxes(title_text=\"month\", row=i+1, col=j+1)\n",
    "            fig.update_yaxes(title_text=\"value\", row=i+1, col=j+1)\n",
    "\n",
    "    # Update layout and display plot\n",
    "    fig.update_layout(\n",
    "        title_text='Cointegration Parameters and Statistics in Monthly Segments',\n",
    "        width=1000,\n",
    "        height=1500\n",
    "    )\n",
    "    iplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cointegration_info_monthly(clean_data[\"usdttmn-nobitex\"], clean_data[\"usdttmn-calculated\"], 'price', cutoff=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cointegration_info_monthly(clean_data[\"usdttmn-nobitex\"], clean_data[\"usdttmn-wallex\"], 'price', cutoff=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cointegration_info_monthly(clean_data[\"usdttmn-wallex\"], clean_data[\"usdttmn-calculated\"], 'price', cutoff=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b702f",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    First, we discuss about <u>Fluctuations in Cointegration Regression Coefficients $\\alpha$ and $\\beta$</u>:\n",
    "    <ol>\n",
    "        <li><b>Cointegration Coefficient \\(\\alpha\\)</b>: \n",
    "            <ul>\n",
    "        <li><strong>Interpretation</strong>: The \\(\\alpha\\) coefficient in a cointegration relationship often represents the intercept or the long-term mean of the relationship.</li>\n",
    "        <li><strong>Fluctuations</strong>: Changes in \\(\\alpha\\) over time could indicate shifts in the long-term equilibrium level between the variables. For instance, in financial markets, this might reflect changes in the mean level of price spreads due to varying market conditions, economic policies, or investor sentiment.</li>\n",
    "        <li><strong>Implications</strong>:\n",
    "            <ul>\n",
    "                <li><strong>Market Dynamics</strong>: Significant shifts in \\(\\alpha\\) may suggest structural changes in the market, such as new regulations, macroeconomic events, or changes in market participant behavior.</li>\n",
    "                <li><strong>Arbitrage Opportunities</strong>: If \\(\\alpha\\) changes while \\(\\beta\\) remains stable, the long-term relationship still holds, but the mean-reversion level has shifted. Traders can exploit this by adjusting their trading strategies to the new mean level.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul></li>\n",
    "        <li><b>Cointegration Coefficient \\(\\beta\\)</b>: \n",
    "            <ul>\n",
    "        <li><strong>Interpretation</strong>: The \\(\\beta\\) coefficient represents the long-term equilibrium relationship between the cointegrated variables.</li>\n",
    "        <li><strong>Fluctuations</strong>: Changes in \\(\\beta\\) suggest alterations in how the variables move together over the long term. For example, in a pair trading strategy, a change in \\(\\beta\\) might indicate a change in the relative pricing of the two assets.</li>\n",
    "        <li><strong>Implications</strong>:\n",
    "            <ul>\n",
    "                <li><strong>Market Dynamics</strong>: Fluctuations in \\(\\beta\\) can indicate changes in the underlying economic fundamentals or the relative strength of the variables involved. It could be due to shifts in supply-demand dynamics, varying risk premiums, or different growth rates.</li>\n",
    "                <li><strong>Arbitrage Opportunities</strong>: If \\(\\beta\\) changes, it might indicate a change in the profit potential of arbitrage strategies. For instance, a higher \\(\\beta\\) could signal increased divergence and, hence, potentially larger profits from mean-reversion trades, while a lower \\(\\beta\\) could signal a stronger co-movement and smaller arbitrage opportunities.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "        </li>\n",
    "    </ol>\n",
    "</font></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18448e",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    Finally, we discuss about <u>Fluctuations in the Stationarity Test Statistic</u>:\n",
    "    <ul>\n",
    "        <li><strong>Stationarity Test Statistic</strong>: This statistic tests whether the residuals of the cointegration relationship are stationary. A significant test statistic indicates that the residuals are stationary, confirming the presence of a cointegration relationship.</li>\n",
    "        <li><strong>Fluctuations</strong>: Variations in the stationarity test statistic over time may indicate changes in the strength of the cointegration relationship. If the test statistic becomes less significant (i.e., the residuals become less stationary), the cointegration relationship might be weakening.</li>\n",
    "        <li><strong>Implications</strong>:\n",
    "            <ul>\n",
    "                <li><strong>Market Dynamics</strong>: Changes in the stationarity test statistic can reflect changes in market volatility, external shocks, or evolving correlations between variables. A weakening cointegration relationship might suggest increased market uncertainty or diverging economic conditions.</li>\n",
    "                <li><strong>Arbitrage Opportunities</strong>: When the stationarity test statistic indicates strong cointegration, it implies robust long-term arbitrage opportunities. Conversely, a weakening test statistic might signal the need for caution as the reliability of mean-reversion trades decreases. Traders might need to adjust their strategies, perhaps by incorporating additional variables or using more sophisticated models to capture the changing dynamics.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</font></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81044777",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color: #3b5996; padding: 10px;border-radius: 25px;\"><center><font face=\"gotham\" color=\"white\">Task 4: Error Correction Model (ECM)</font></center><h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1abdd",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">1. ECM Development:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85093ccb",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    In the begining, we name the <u>Key Reasons for Using <b>ECM</b></u>:\n",
    "     <ul>\n",
    "        <li><strong>Cointegration Presence</strong>: The presence of cointegration between variables indicates a long-term equilibrium relationship, which the ECM can effectively model.</li>\n",
    "        <li><strong>Short-Term Adjustments</strong>: ECM captures the short-term dynamics and adjustments needed to correct deviations from the long-term equilibrium.</li>\n",
    "        <li><strong>Model Parsimony</strong>: ECMs provide a parsimonious way to incorporate both short-term fluctuations and long-term relationships without overfitting.</li>\n",
    "    </ul>\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06531f1c",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    Next, we explain the <u>Steps Involved in <b>ECM</b> Construction</u>:\n",
    "     <h3>1. Identify Cointegrated Pairs</h3>\n",
    "    <p>Perform cointegration tests (We implement two step Engle-Granger test) on the time series data to identify pairs of variables that are cointegrated. This step confirms the existence of a long-term equilibrium relationship between the variables.</p>\n",
    "    <h3>2. Estimate Long-Term Relationship</h3>\n",
    "    <ol>\n",
    "        <li>Use Ordinary Least Squares (OLS) regression to estimate the long-term equilibrium relationship between the cointegrated variables. The regression model is typically of the form:\n",
    "            \\[\n",
    "Y_t = \\alpha + \\beta X_t + \\epsilon_t\n",
    "\\]\n",
    "            where \\(Y_t\\) and \\(X_t\\) are the cointegrated variables, \\(\\alpha\\) and \\(\\beta\\) are the regression coefficients, and \\(\\epsilon_t\\) is the residual term.</li>\n",
    "    </ol>\n",
    "    <h3>3. Obtain Residuals (Error Term)</h3>\n",
    "    <ol>\n",
    "        <li>Calculate the residuals (\\(\\epsilon_t\\)) from the long-term relationship estimated in the previous step. These residuals represent the deviations from the long-term equilibrium.</li>\n",
    "    </ol>\n",
    "    <h3>4. Construct the ECM</h3>\n",
    "    <ol>\n",
    "        <li>Formulate the ECM to capture both short-term dynamics and the long-term equilibrium adjustment. The simple ECM typically has the form:\n",
    "            \\[\n",
    "\\Delta Y_t = \\gamma + \\phi \\epsilon_{t-1} + \\delta_0 \\Delta X_{t} + \\eta_t\n",
    "\\]\n",
    "            where:\n",
    "            <ul>\n",
    "                <li>$\\Delta$ represents the difference operator (i.e., \\(\\Delta Y_t = Y_t - Y_{t-1}\\))</li>\n",
    "                <li>$\\gamma$ is the intercept</li>\n",
    "                <li>$\\phi$ is the error correction coefficient</li>\n",
    "                <li>$\\epsilon_{t-1}$ is the lagged residual term (error term) from the long-term relationship</li>\n",
    "                <li>$\\delta_0$ is the coefficient for the difference of the independent variables($X$)</li>\n",
    "                <li>$\\eta_t$ is the white noise error term</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ol>\n",
    "    <h3>5. Estimate the ECM Parameters</h3>\n",
    "    <ol>\n",
    "        <li>Use regression techniques to estimate the parameters of the ECM. This involves regressing the differenced dependent variable ($\\Delta Y_t$) on the lagged error term ($\\epsilon_{t-1}$) and the differenced independent variable ($\\Delta X_{t}$).</li>\n",
    "    </ol>\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee39aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_error_correction_model(X, Y):\n",
    "    \"\"\"\n",
    "    Perform a simple error correction model (ECM) between two time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X, Y : pd.Series\n",
    "        Time series data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sm.regression.linear_model.RegressionResultsWrapper\n",
    "        Fitted linear regression model for the ECM.\n",
    "    \"\"\"\n",
    "    # Perform cointegration test and get residuals\n",
    "    const, X, Y, resid = cointegration_test(X, Y, return_mode=\"variables+resid\")\n",
    "    \n",
    "    # Create a DataFrame for ECM data with differenced series and lagged residuals\n",
    "    ecm_data = pd.DataFrame({\n",
    "        'X_diff': X.diff(),\n",
    "        'Y_diff': Y.diff(),\n",
    "        'disequilibrium': resid.shift(1),\n",
    "        'const': const\n",
    "    }).dropna()  # Drop rows with NaN values\n",
    "    \n",
    "    # Define predictors\n",
    "    predictors = ['X_diff', 'disequilibrium', 'const']\n",
    "    \n",
    "    # Fit the ECM using OLS regression\n",
    "    lr_ecm_model = sm.OLS(ecm_data['Y_diff'], ecm_data[predictors])\n",
    "    lr_ecm_model_fit = lr_ecm_model.fit()\n",
    "    \n",
    "    return lr_ecm_model_fit\n",
    "\n",
    "def error_correction_coefficient_all_frequencies(data, exchange_pair1, exchange_pair2, col='price'):\n",
    "    \"\"\"\n",
    "    Calculate the error correction coefficient for all frequencies.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        Dictionary containing dataframes for different frequencies.\n",
    "    exchange_pair1, exchange_pair2 : str\n",
    "        Names of the exchange pairs.\n",
    "    col : str, optional\n",
    "        Column name for which to compute the error correction coefficient (default is 'price').\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the error correction coefficients for all frequencies.\n",
    "    \"\"\"\n",
    "    disequilibrium_coefficients = {}\n",
    "    \n",
    "    # Iterate over each frequency\n",
    "    for freq in data[exchange_pair1].keys():\n",
    "        # Get the series for the given frequency\n",
    "        X = data[exchange_pair1][freq][col]\n",
    "        Y = data[exchange_pair2][freq][col]\n",
    "        \n",
    "        # Calculate the ECM and extract the disequilibrium coefficient\n",
    "        ecm_model_fit = simple_error_correction_model(X, Y)\n",
    "        disequilibrium_coefficients[freq] = [ecm_model_fit.params['disequilibrium']]\n",
    "        \n",
    "    # Create a DataFrame from the coefficients\n",
    "    return pd.DataFrame(data=disequilibrium_coefficients, index=[f'ECM coefficient for X = {exchange_pair1}, Y = {exchange_pair2}'])\n",
    "\n",
    "def error_correction_coefficient_pair(data, exchange_pair1, exchange_pair2):\n",
    "    \"\"\"\n",
    "    Calculate the error correction coefficients for a pair of exchange pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        Dictionary containing dataframes for different frequencies.\n",
    "    exchange_pair1, exchange_pair2 : str\n",
    "        Names of the exchange pairs.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the error correction coefficients for the exchange pairs.\n",
    "    \"\"\"\n",
    "    # Calculate the ECM coefficients for both directions\n",
    "    df1 = error_correction_coefficient_all_frequencies(data, exchange_pair1, exchange_pair2)\n",
    "    df2 = error_correction_coefficient_all_frequencies(data, exchange_pair2, exchange_pair1)\n",
    "    \n",
    "    # Concatenate the results into a single DataFrame\n",
    "    return pd.concat([df1, df2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_correction_coefficient_pair(clean_data, \"usdttmn-nobitex\", \"usdttmn-wallex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69854b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_correction_coefficient_pair(clean_data, \"usdttmn-nobitex\", \"usdttmn-calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0089f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_correction_coefficient_pair(clean_data, \"usdttmn-wallex\", \"usdttmn-calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866165f3",
   "metadata": {},
   "source": [
    "<h3><font face=\"gotham\" color=\"#3b5996\">2. Analysis of Reversion Dynamics:</font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700de4a7",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    First, we discuss about the <u>Significance of Error Correction Coefficient($\\phi$)</u>:\n",
    "     <ul>\n",
    "        <li><strong>Interpretation</strong>:\n",
    "            <ul>\n",
    "                <li>If \\(\\phi\\) is positive and significant, it suggests that deviations from equilibrium are corrected in the next period.</li>\n",
    "                <li>If \\(\\phi\\) is negative and significant, it indicates overshooting, where deviations are initially corrected more than necessary, followed by a gradual return to equilibrium.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><strong>Implications</strong>:\n",
    "            <ul>\n",
    "                <li><strong>Quick Adjustment</strong>: A significant \\(\\phi\\) implies that the market quickly adjusts to restore equilibrium, presenting potential trading opportunities for mean-reversion strategies.</li>\n",
    "                <li><strong>Market Efficiency</strong>: Significant \\(\\phi\\) suggests that market forces efficiently correct any imbalances between the variables, making it challenging to exploit persistent deviations from equilibrium.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2383cb",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    Now we explain the <u>Speed of Reversion</u>:\n",
    "     <p>The magnitude of the error correction coefficient (\\(\\phi\\)) provides insights into the speed of reversion. A larger absolute value of \\(\\phi\\) indicates a faster rate of reversion, while a smaller absolute value implies a slower adjustment process.</p>\n",
    "    <ul>\n",
    "        <li><strong>Interpretation</strong>:\n",
    "            <ul>\n",
    "                <li>A large positive \\(\\phi\\) indicates rapid adjustment towards equilibrium, reflecting strong reversion mechanisms.</li>\n",
    "                <li>A small positive \\(\\phi\\) suggests a slower rate of adjustment, indicating that the market takes longer to correct deviations.</li>\n",
    "                <li>A negative \\(\\phi\\) (overshooting) implies that corrections initially exceed the equilibrium level, followed by a gradual return.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><strong>Implications</strong>:\n",
    "            <ul>\n",
    "                <li><strong>Trading Strategies</strong>: The speed of reversion guides trading strategies. Faster reversion may lead to shorter holding periods for trades, while slower reversion may require longer-term positions.</li>\n",
    "                <li><strong>Risk Management</strong>: Understanding the speed of reversion helps manage risk associated with mean-reversion trading strategies. Faster reversion reduces exposure to prolonged drawdowns, while slower reversion requires careful risk management to withstand extended periods of deviation from equilibrium.</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad50155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ECM_coefficient_plot(monthly_segments1, monthly_segments2, freq):\n",
    "    \"\"\"\n",
    "    Create a Plotly trace for the ECM coefficients across months.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    monthly_segments1, monthly_segments2 : dict\n",
    "        Dictionaries containing monthly segmented dataframes.\n",
    "    freq : str\n",
    "        Frequency identifier for labeling.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    go.Scatter\n",
    "        Plotly trace for ECM coefficients.\n",
    "    \"\"\"\n",
    "    info_monthly = np.zeros((1, 12))\n",
    "    \n",
    "    # Iterate over each month and compute ECM coefficient\n",
    "    for i, month in enumerate(monthly_segments1.keys()):\n",
    "        series1 = monthly_segments1[month]\n",
    "        series2 = monthly_segments2[month]\n",
    "        ecm_coefficient = simple_error_correction_model(series1, series2).params.iloc[1]\n",
    "        info_monthly[0][i] = ecm_coefficient\n",
    "    \n",
    "    # Create a Plotly trace\n",
    "    trace = go.Scatter(\n",
    "        x=list(monthly_segments1.keys()),\n",
    "        y=info_monthly[0],\n",
    "        mode=\"lines+markers\",\n",
    "        name=f'time-frame-interval = {freq}'\n",
    "    )\n",
    "      \n",
    "    return trace\n",
    "\n",
    "def plot_ECM_coefficient_monthly_pair(data, exchange_pair1, exchange_pair2, col):\n",
    "    \"\"\"\n",
    "    Plot ECM coefficients for monthly segments across multiple time frames.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        Dictionary containing dataframes for different frequencies.\n",
    "    exchange_pair1, exchange_pair2 : str\n",
    "        Names of the exchange pairs.\n",
    "    col : str\n",
    "        Column name for which to compute ECM coefficients.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    subplot_titles = [\n",
    "        f\"ECM coefficient for X = {exchange_pair1}, Y = {exchange_pair2}\",\n",
    "        f\"ECM coefficient for X = {exchange_pair2}, Y = {exchange_pair1}\"\n",
    "    ]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = subplots.make_subplots(rows=2, cols=1, subplot_titles=subplot_titles)\n",
    "\n",
    "    # Iterate over each frequency\n",
    "    for i, freq in enumerate(data[exchange_pair1].keys()):\n",
    "        \n",
    "        # Split the series into monthly segments\n",
    "        monthly_segments1 = split_series_to_monthly_segments(data[exchange_pair1][freq][col])\n",
    "        monthly_segments2 = split_series_to_monthly_segments(data[exchange_pair2][freq][col])\n",
    "        \n",
    "        # Create ECM coefficient plots\n",
    "        trace1 = create_ECM_coefficient_plot(monthly_segments1, monthly_segments2, freq)\n",
    "        fig.add_trace(trace1, row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"Month\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Value\", row=1, col=1)\n",
    "        \n",
    "        trace2 = create_ECM_coefficient_plot(monthly_segments2, monthly_segments1, freq)\n",
    "        fig.add_trace(trace2, row=2, col=1)\n",
    "        fig.update_xaxes(title_text=\"Month\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Value\", row=2, col=1)\n",
    "\n",
    "    # Update layout and display plot\n",
    "    fig.update_layout(title_text='ECM Coefficients in Monthly Segments')\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd7cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ECM_coefficient_monthly_pair(clean_data, \"usdttmn-nobitex\", \"usdttmn-calculated\", 'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e6c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ECM_coefficient_monthly_pair(clean_data, \"usdttmn-nobitex\", \"usdttmn-wallex\", 'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368f4972",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ECM_coefficient_monthly_pair(clean_data, \"usdttmn-wallex\", \"usdttmn-calculated\", 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c046b",
   "metadata": {},
   "source": [
    "<p style=\"font-size:17px\"><font face=\"gotham\">\n",
    "    Splitting the dataset into monthly segments and applying the ECM to each segment allows for a dynamic analysis of the evolving relationships between cointegrated variables. By observing how the ECM parameters change over time, we gain insights into the shifting dynamics of the market: <ul>\n",
    "        <li><strong>ECM Parameters Fluctuation</strong>: Analyze how the error correction coefficient (\\(\\phi\\)) and other parameters vary across different monthly segments. Are there consistent patterns or significant changes over time?</li>\n",
    "        <li><strong>Market Adaptation</strong>: Assess how the market adapts to short-term shocks and long-term trends. Do the speed and significance of reversion mechanisms change during volatile periods or stable market conditions?</li>\n",
    "    </ul></font></p>\n",
    "   <p style=\"font-size:17px\"><font face=\"gotham\">Also, Understanding the evolving dynamics revealed by the ECM has several implications for identifying short-term market inefficiencies:\n",
    "    <ul>\n",
    "        <li><strong>Temporal Variations in Inefficiencies</strong>: Identify periods of increased or decreased market inefficiencies based on the ECM analysis. For example, if the error correction coefficient becomes less significant or decreases in magnitude, it may signal a period of reduced market efficiency.</li>\n",
    "        <li><strong>Exploitable Deviations from Equilibrium</strong>: Look for instances where the market deviates significantly from the long-term equilibrium predicted by the ECM. These deviations represent potential trading opportunities, as markets tend to revert to their equilibrium levels over time.</li>\n",
    "    </ul></font></p>\n",
    "    <p style=\"font-size:17px\"><font face=\"gotham\">Based on the observed dynamics and identified inefficiencies, we can consider potential trading strategies that could exploit these inefficiencies:\n",
    "    <ul>\n",
    "        <li><strong>Mean-reversion Strategies</strong>: Utilize the insights from the ECM to develop mean-reversion strategies that capitalize on short-term deviations from equilibrium. Enter trades when the market overreacts to news or events, expecting a subsequent correction.</li>\n",
    "        <li><strong>Pairs Trading</strong>: Identify pairs of assets that exhibit cointegration and use the ECM to refine pairs trading strategies. Trade the spread between the assets when it diverges significantly from its historical relationship, anticipating a return to equilibrium.</li>\n",
    "        <li><strong>Dynamic Hedging</strong>: Adjust hedging strategies based on the evolving dynamics revealed by the ECM. Hedge positions more aggressively during periods of increased market inefficiencies to mitigate short-term risks.</li>\n",
    "    </ul>\n",
    "</font></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
